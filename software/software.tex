\begin{abstract}
  The ability to interrogate data, to reproduce methods, and to
  improve techniques are cornerstones of scientific research.  Free
  software is an important resource in the increasingly data driven
  quantitative revolution in contemporary molecular cell biology
  research.  We wished to implement a fully transparent and
  reproducible vision of a build process referred to as ``reproducible
  research'' that directly links primary data resources such as
  sequence databases and microscopy images with the standard
  expression of scientific research insights as manuscripts and
  theses.  To achieve this we contributed to a number of free software
  projects including the Octave programming language, the BioPerl
  suite of scientific data tools, and the Debian GNU/Linux operating
  system.  This included implementing new algorithms, refactoring code
  for efficiency and consistency, adding new function libraries,
  creating supporting plugin tools, and packaging software for ease of
  installation by non-expert users.  The software developed and
  contributed to public repositories should enable other researchers
  to reproduce our work, and to implement a diverse variety of their
  own solutions in computational biology.
\end{abstract}

Today's biology research is mainly digital.  Data being generated is so
complicated and extensive that analysis is done by software, with computer data
acquisition and analysis as an integral part of a researcher's work.
Such work is then published, but it is only useful to the scientific
community if it can be reproduced by colleagues and further
research can be performed to extend from it.  Even if the software used is
carefully described in the methods, modern programs are so
complicated, and analysis is so dependent on so many variables, that
there is limited hope for other scientists to recreate the
implementation from brief descriptions.  This
makes access to the original software used in research a prerequesite for
reproducibility.

Sharing the code is not enough.  Programs that undertake the direct analysis
tasks sit on top of several other components.  Specific versions of
libraries are used, on top of a specific programming language, running
in a specific operating system on hardware with a particular
specification.  Reproducing this runtime environment is not trivial.

Following from free software came the ideals of open and collaborative
software development.  The seminal essay ``The Cathedral and the
Bazaar'' by \citet{raymond1999-cathedral-and-bazaar} compares the
methods of free software development by an exclusive group, in a
``Cathedral'' where the source code is only made public for releases,
with a method where development
happens in the public domain, a ``bazaar'', where anyone can see
and contribute to it.
In free software, the source code for a program is available on the
internet for anyone.  The ``bazaar'' takes this further by having every
single change open on the internet in real time, even before a new version
of the software is formally released.  This invites users to contribute
through discussion about development in public forums.
The ideal is that any user can become a
developer, simply by sending their contributions to the appropriate public
forum.  Users are motivated to do this because they
have a personal interest in enabling the
program to support a specific feature that matters to them.  And they
are incentivised to contribute to the project because it is less work
to participate in the collective
than to write their own project just to have the feature
they lack.
Free software projects \citep{schindelin2012fiji, bioperl} are now
now almost all developed using the ``bazaar'' approach.

We faced many computational challenges
during our research presented in \Cref{ch:histone-catalogue}
and \Cref{ch:kill-frap}.
Using only free software we were able to contribute our work to several
free software projects in collaborations that go beyond the
boundaries of the typical research group.  While the advances allowed
us to perform the acquisition and analysis of our own data,
we expect that these open contributions
will also enable other researchers to advance their own research.

\section{GNU Octave}

The GNU Octave programming language was used extensively for the analysis
of microscope images in \Cref{ch:kill-frap}.  Several features of the
language such as its handling of multi-dimensional arrays, interactive
user interface, and extensive image processing functions, as well as
its supportive community, make it an attractive choice
for quantitative microscopy.

%% All values are multi-dimensional arrays does not include objects
%% wrapped from other languages such as java objects (and I guess it
%% will be the same for python when the pytave project is finished).
GNU Octave is a high level array programming
language where operations are generalised to multidimensional arrays
and Octave is primarily intended for numerical computations \citep{octave}.
All values are multidimensional arrays and this provides
an abstraction layer that is useful when writing code for an arbitrary
number of dimensions.

%% Doesn't seem to be any scientific study about REPL as an advantage
%% for prototyping.  Similarly, none about being better for
%% exploratory data analysis but I can't imagine anyone disagreeing
%% with it.
Octave also has a Read--Eval--Print Loop (REPL), or interactive top
level user interface, similar to the IPython and Unix shells.  This reduces the
feedback time and provides an efficient
environment for exploratory data analysis.

%% No citation for this, only the anecdotal evidence of being the
%% project leader of octave Forge for 6 years.
The project has an active community
mainly composed of
scientists and engineers who provide a large support group of
specialists in numerical computations.

The separate Octave Forge project hosts a large number of
packages that extend Octave into specific applications such as control
systems \citep{octave-control}, time-frequency analysis
\citep{octave-ltfat}, or level sets \citep{octave-level-set}.
It provides a collaborative environment for development of Octave
packages and another nexus of the Octave community.

Both GNU Octave and Octave Forge packages are free software which allows
the study, modification, and distribution of modified source.
We made extensive
use of this feature and have contributed to improving Octave and its
packages for the needs of quantitative microscopy.

Finally, Octave comes with basic support for image processing and the
Octave Forge Image package extends this with a number of functions for
image processing such as geometric transformations, mathematical
morphology, image registration, and noise reduction.

%% These were actually lsm files but those are actually TIFFs.
While Octave is well suited to image
processing, we identified several problems related to
large image size or number of dimensions.

Our microscopy images
were several megabytes in size.  For example, the single cell
FRAP experiments for histone dynamics generating
TIFF files of \SI{489}{\mebi\byte}.
These images had a field of view of 300
by 300 pixels and 2500 time frames
on acquisition by an 8-bit camera.
Such files sizes are typical in the microscopy field.

Biological microscope images have a varying number of dimensions.
In addition to the 2 dimensional plane of a standard image, microscope
outputs typically include any combination of $z$ dimension along
the optical axis for a volume image; time dimension for time-lapse
experiments; and wavelength for multi-channel experiments.  Recent microscopy
techniques generate images with additional dimensions
such as angle, phase, and lifetime.  This varying dimensionality adds
complexity and makes it challenging to write generalised subroutines for
microscope image analysis.

\subsection{Reading and Writing of Image Files}

%% Number of 88 major image formats taken from GraphicsMagick
%% documentation on February 2017.
Octave uses the GraphicsMagick C++ library for the reading and writing
of image files which provides a common interface
to a varied collection of
image format specific libraries covering almost
90 major image formats.  In Octave, this functionality is
provided via the functions \command{imfinfo}, \command{imread}, and
\command{imwrite}.

%% The importance of reading all planes in a single call to imread is
%% of performance for TIFF.  This is because of the structure of TIFFs
%% where reading plane N means parsing all the IFDs before that.
We rewrote these three functions with the aim of achieving
reduced memory usage, increased performance, an improved
interface for multidimensional images, and new image types.
New options were added to read and write a series of image planes
in a single function call, to read specific regions of interest in
individual planes, to read and write images
with floating point precision, and to append additional planes to existing image
files.

As part of this rewrite, new features were introduced such as
support for the CMYK colour model, EXIF
and GPS metadata, reading and writing of animations in GIF images, and
control over the image compression type,
while existing features were improved such as
support for transparency and
indexed images.

%% We can't use the term image IO functions because then we would have
%% to explain what IO is.  For the same reason, we can't say ``hooks''.
To support this development, we created a system that allows changing
how the \command{imread}, \command{imwrite}, and \command{imfinfo}
functions behave for each file format.
This enables Octave extensions to add support for new image file
formats or to improve access to the existing formats
without addition of new format specific functions.
For example, it is now possible for packages to enable reading of
microscope specific metadata from \command{imfinfo}.
This system is available via the function \command{imformats}.

Changes were required in other Octave functions related to images
including those involved in conversion between color
models, grayscale images, and indexed images to support integer and
floating point data types, and multiple dimensions
\trefp{tab:software:octave-core-functions}.

All changes were released with Octave version 3.8.0.

\begin{table}
  %% This table includes the major things we contributed to Octave
  %% core.  It doesn't mention a version since it's work I contributed
  %% between Octave 3.6 and 4.2.  Some of the changes even span
  %% multiple versions.
  %% It does not include minor bug fixes or trivial bug fixes.  Also
  %% does not include major changes we did to the internals which only
  %% reduced code duplication.
  \captionIntro{New or improved functions in GNU Octave}{}
  \label{tab:software:octave-core-functions}
  \begin{multicols}{3}
    \begin{itemize}[label={}]
      \foreach \function in {
        bitcmp,
        bzip2,
        cubehelix,
        fliplr,
        flipud,
        flip,
        frame2im,
        gallery,
        gray2ind,
        gzip,
        hsv2rgb,
        im2double,
        im2frame,
        imfinfo,
        imformats,
        imread,
        imwrite,
        ind2gray,
        ind2rgb,
        inputParser,
        ntsc2rgb,
        pkg,
        psi,
        rectint,
        rgb2hsv,
        rgb2ind,
        rgb2ntsc,
        rot90,
        validateattributes}
      { \item \command{\function} }
    \end{itemize}
  \end{multicols}
\end{table}

\subsection{Bio-Formats and Octave Java interface}

%% We kind of managed to read the pixel data.  This also required
%% knowing that LSM interleaves the actual pixel data with thumbnails
%% which need to be skipped.
While GraphicsMagick provides support to read many image formats, its
support for scientific microscope image formats is limited.
For example, Zeiss confocal microscopes saves
images in the LSM file format which is a proprietary extension of TIFF.
Although GraphicsMagick reads LSM pixel data,
the file metadata such as pixel size, time interval, or region of
bleaching event cannot be retrieved.

Bio-Formats is a free software library for reading and writing image
data with a strong focus on microscopy image file formats
\citep{bioformats}.  It is written in the Java programming language and
used by other programs in the field of microscope image analysis such
as CellProfiler \citep{cellprofiler}, ImageJ \citep{imagej2}, and OMERO
\citep{omero}.

%% Before version 3.8.0, there was a separate Octave Forge package
%% that added the java interface.  Actually, Octave version 3.8.0
%% pretty much only merged the java package into it.
Octave has a native interface to the Java programming language
that should enable easy integration with
Bio-Formats, and Bio-Formats has a Matlab toolbox that should be
compatible with Octave.
We identified a series of problems in the Octave interface to Java that
could be solved either by improving Octave or Bio-Formats.

%% Writing this paragraph without mentioning the words data type,
%% primitive, wrapper, and objects is though.  And on top of that,
%% writing it without sounding dumb is just impossible.
In Octave, we rewrote the handling of values returned from Java.
Of special importance for BioFormats integration was the automatic conversion of
Java arrays which are used to return pixel data.
Support for conversion of multidimensional Java arrays was not implemented
since we did not require it and this remains open as a future project.

In Bio-Formats, we modified the Matlab toolbox so that it is
both Octave and Matlab compatible.
This change simplifies the packaging of
Bio-Formats for Matlab and Octave making them
effectively the same code.
Finally, we automated the creation of Octave packages in Bio-Formats
so that they can be made available as part of standard Bio-Formats releases.

All changes were released with Octave since version 4.0.0 and
Bio-Formats since version 5.1.2

\section{Octave Forge Image package}

The Octave programming language is primarily intended for numerical
computations and provides a syntax and set of functions that is particularly
convenient for solving linear algebra and differential equations.  It
also includes a collection of functions for the handling of image data
but these are focused on the reading and writing of image files,
conversion between colour models, and graphical display.

The Octave Forge Image package supplements Octave with a wide range of
specialised functions image analysis.  While Octave syntax is
identical for any number of dimensions and data type, we found that
many of the Image package functions were either limited to two
dimensional images or inefficient for the large images such as those used in
microscopy.  We began an effort to eliminate any
limitation on the number of dimensions in the Image package
\trefp{tab:software:octave-image-functions}.  This has seen multiple
releases, starting in version 2.0.0 and continuing to the current
version 2.6.1.
%% Other issues are when functions just treated higher number of
%% dimensions as a long 2D image and introduced artifacts on image
%% borders.  Or they looped over each pixel making it terribly slow
%% even on 2d images.
%%
%% The first time the project to support ND images was made official
%% was for the summer of 2013 (my GSOC) but even on that proposal I
%% mentioned work already made.

\begin{table}
  \captionIntro{New or improved functions in the Octave Forge Image
    package}{}
  \label{tab:software:octave-image-functions}
  \begin{multicols}{3}
    \begin{itemize}[label={}]
      \foreach \function in {
        bestblk,
        bwareafilt,
        bwareaopen,
        bwconncomp,
        bwdist,
        bwlabel,
        bwlabeln,
        bwmorph,
        bwperim,
        bwpropfilt,
        checkerboard,
        col2im,
        colfilt,
        conndef,
        edgetaper,
        fftconv2,
        fftconvn,
        grayslice,
        graythresh,
        im2col,
        imabsdiff,
        imadjust,
        imattributes,
        imbothat,
        imclearborder,
        imclose,
        imcomplement,
        imcrop,
        imdilate,
        imerode,
        imfill,
        imgetfile,
        imhist,
        imlincomb,
        immse,
        imopen,
        impixel,
        impyramid,
        imquantize,
        imreconstruct,
        imregionalmax,
        imregionalmin,
        imresize,
        imrotate,
        imtophat,
        intlut,
        iptcheckconn,
        label2rgb,
        labelmatrix,
        mat2gray,
        mmgradm,
        montage,
        nlfilter,
        normxcorr2,
        ordfiltn,
        otf2psf,
        padarray,
        psf2otf,
        psnr,
        regionprops,
        rgb2ycbcr,
        strel,
        stretchlim,
        subimage,
        tiff\_tag\_read,
        watershed,
        wavelength2rgb,
        ycbcr2rgb}
      { \item \command{\function} }
  \end{itemize}
  \end{multicols}
\end{table}

\subsection{Image Thresholding Algorithms}

Image thresholding is a method for image segmentation where the image
pixels are separated into classes based on their intensity values.
The simplest of thresholding methods uses a single fixed value and
separates an image into background and foreground which are then
represented in a binary black or white image.  This is
particularly useful for fluorescence microscopy images where objects
of interest show bright high intensity values against a dark
background of low intensity values.

Multiple threshold algorithms exist to automate the choice of a
threshold value, the most common of which is based on the analysis
of an image histogram \citep{analysis-hist-threshold}.  Otsu's method
\citep{otsu-threshold} is among the most widely used approach
and is available in the function \command{graythresh} in the Image
package.  We rewrote this function to handle histograms of arbitrary
length and vectorised it for performance with additional computation of a
``goodness'' measure of threshold value optimality.
In addition, an unpublished collection of histogram
threshold algorithms by Antti Niemistö (personal communication)
was made available as an
option for \command{graythresh}.  Finally, the rewrite
of \command{graythresh} added the
option of using a histogram as input instead of an image, enabling
the possibility of histogram processing as a preparatory step.

%% We have no reference for Antti Niemistö work.  It was never
%% published and only available on his University webpage at
%% http://www.cs.tut.fi/~ant/histthresh/ which is no longer available.
%% He has several other publications but none about this topic.
%% Here's one: A. Niemistö, V. Dunmire, O. Yli-Harja, W. Zhang, and
%% I. Shmulevich, "Robust quantification of in vitro angiogenesis
%% through image analysis," IEEE Transactions on Medical Imaging,
%% vol. 24, no. 4, pp. 549-553, April 2005.
%%
%% The only copies of his work that I know of are on my github account
%% (which I have since modified), and on https://archive.org/web

All these changes were released in the Octave Forge Image
package version 2.0.0.

\subsection{Mathematical Morphology}

%% This is not the complete story.  When we started, imdilate and
%% imerode had some support for ND images.  I can't quite remember
%% what was missing but I do remember making a table at the end with
%% performance comparison, and some comparisons couldn't be made
%% because they were missing before.  Looking at the source for 1.0.15
%% (before I started this), I can see that imerode and imdilate called
%% ordfiltn for grayscale images which was bound to be quite slow, and
%% filter2 for binary images which would limit to 2d images.
Mathematical morphology is the analysis of spatial
structures, and this provides a powerful image analysis technique based on
the shape and form of objects.  It is achieved by probing an image
with a known shape called the Structuring Element (SE), and filtering the
image based on whether the SE fits at each location
within the image.
Mathematical morphology is a very relevant tool for identifying cell
locations and features in microscopy images.

The fundamental operations of mathematical morphology are named dilation and
erosion.  These are available in the Image package through the
\command{imdilate} and \command{imerode} functions.  More complex
morphological operations are built on top of these two operations.  For
example, the morphological top-hat transform corresponds to the difference
between an image and its morphological opening, which in turn
corresponds to an erosion followed by a dilation.

As the two fundamental operations, dilation and erosion are ideal targets for
improvement since any performance increase or
support for new image types will be propagated to
higher level morphology functions.

%% The original imdilate and imerode used __spatial_filtering__ with
%% the ordefiltn option.  We first optimised this by using convn which
%% made it a lot faster but only for binary images.  We then modified
%% __spatial_filtering__ so that it was faster in grayscale images
%% too.  We ended up writing a specialised function for dilation and
%% erosion.

%% My notes say that perfomance gains were between 1.5-30x for erosion
%% and dilation, and 1/5 to 2/5 faster for __spatial_filtering__.  At
%% least the erosion and dilation were on my old blog which got lost
%% after the server was damaged.
%% SE specialisation == SE decomposition
\command{imdilate} and \command{imerode} used the general purpose
\command{\_\_spatial\_filtering\_\_} function of the Image package.
We rewrote the functions aimed at morphology operations with
specialisations for different data types and SE.
A new \command{strel} class was created for the SE specialisation which
supported both flat and non-flat SE.

These changes were released with
the Octave Forge Image package version 2.2.0.

%% Not mentioning the rewrite of __spatial_filtering__ because the
%% only reason to do so was performance.  While my notes mention nice
%% values, I don't have the time to make proper performance comparison
%% so better not even talk about it.

\subsection{Image Regions of Interest}

The function \command{regioprops} is used to measure different
properties of regions of interest in an image such as its centroid,
area, or eccentricity.  However, the function was inappropriate for
the measurement of multiple regions and properties because the whole
image was analysed independently for each region and property.  We rewrote
this function for improved efficiency so that
computation of area and intensity weighted
centroid of 2000 regions which previously took over 12 hours on a
standard desktop computer was finish in under 3 minutes.
This improvement was released with the Octave Forge Image package version 2.6.0.
%% We say over 12 hours but actually they never finished.  Not having
%% finished after so many hours (overnight) was absolutely not
%% acceptable.  Estimates say that it would actually take over ~200
%% hours to finish.
%% The images I used for testing were Ezequiel's histone modification
%% markers, the images were 512x512x30 with slightly more than 2000
%% very small objects.

%% Actually there was a version of bwconncomp before but it didn't do
%% this, it only identified the boundaries of each object.  So while
%% technically, this function name existed, it was something else.
%% Our first attempt made use bwlabeln but then it had to undo the
%% labelling which is inefficient.  That was released with version
%% 2.2.0.  The writing of it as a function performing flood-fill was
%% done later and released with image 2.4.0.
We also wrote the function \command{bwconncomp} to perform the
identification of image regions with an arbitrary number of
dimensions.
\command{bwconncomp} is used internally by \command{regionprops} and
creates an array of indices for the image regions as an
alternative to labelled images which reduces memory usage.
This was released with the Octave Forge Image
package version 2.4.0.

%% Other Octave that I am not mentioning:
%%   * move all XMatrix as subclasses of XNDarray to reduce code
%%     duplication.
%%  * liboctave better integration with STL
%%  * templates for svd instead of duplicated classes
%%  * cleanup of methods (capacity, length, nzmax)
%%  * making Octave quiet when calling scripts of eval.
%%  * zero length dimensions on broadcasting
%%  * accumarray, zeros, ones, etc with empty lists
%%  * rewrite 'pkg build' for distribution of binary packages.
%%  * miscellaneous package: textable and units
%%  * signal package: xcorr2 coeff option
%%  * statistics: hist3, grp2idx, squareform, and binostat
%%  * mapping: a bunch of low hanging fruits
%%  * dicom: add support for any version of gdcm.
%%
%%  * all the work on the zenity package.  Maybe I will still write
%%    about it...
%%  * normxcorr2 in the image package.

\section{Octave FRAP package}
\label{sec:software:octave-frap}

For the FRAP analysis of histones
in \Cref{ch:kill-frap} we required tools to estimate
binding constants from FRAP recovery data.  We obtained the code for a
previously reported circle FRAP model \citep{mcnally-frap-code} by
personal communication with the authors (under the GNU General Public
Licence (GPL) version~3 or later).  This model includes multiple
biophysical parameters including the profile of the photobleach, correction
for observational photobleaching, finite size of the nucleus, and
fitting to both a pure-diffusion model and a full model with binding
states.

This code was written in the Matlab programming language so was
easily ported to Octave.  The main difference was the replacement of
the nonlinear fitting from \command{nlinfit} with \command{leasqr}
from the Octave Forge Optim package since both perform the same
Levenberg--Marquardt nonlinear least squares algorithm.
To validate our port to Octave, we compared the results obtained with
our Octave port against the results obtained by the original authors
in Matlab.  We picked three of our datasets and both implementations
returned the same results.

Identification of the bleach spot, nucleus, and background regions are
required for the circle FRAP model.  The bleach spot analysis measures
intensity recovery and also models the photobleach profile since it
takes into account a non-uniform spatial distribution of the bleached
spot.  The nucleus region analysis defines the finite sized nucleus and takes
into account the fluorescence loss due to observational
photobleaching.  A small region outside the nucleus is used for
background correction.

We automated the identification of all these regions to facilitate
batch processing.  The bleach spot was
identified from the difference between the post and pre-bleach frames.
Individual nuclei were then segmented after automatic thresholding with Otsu's
method.  A background region was identified as the rectangle of a
fixed size with lowest average intensity in the image.

\begin{figure}
  \centering
  \subbottom[pre-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-prebleach.png}
                    \label{subfig:software:frap-roi:prebleach}
  }
  \hfill
  \subbottom[post-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-postbleach.png}
                    \label{subfig:software:frap-roi:postbleach}
  }
  \subbottom[pre-bleach $-$ post-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-subtracted.png}
                    \label{subfig:software:frap-roi:subtracted}
  }
  \hfill
  \subbottom[Identified ROIs]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-selected.png}
                    \label{subfig:software:frap-roi:selected}
  }
  %% These images are not the visual logs because then we would not be
  %% able to have them as separate subfigures or add a scalebar.
  \captionIntro{Automatic selection of regions for FRAP analysis}
               {HeLa stable cell line expressing the H4~R45H mutant
                 tagged with YFP were imaged every \SI{30}{\ms} in a
                 confocal microscope. A circular shape is used for
                 photobleaching after 100~frames.
                 \subcaptionref{subfig:software:frap-roi:prebleach}
                 averaging of 50~pre-bleach images removes most of the
                 noise, allowing for a better refined ROI;
                 \subcaptionref{subfig:software:frap-roi:postbleach}
                 average of 5 post-bleach images;
                 \subcaptionref{subfig:software:frap-roi:subtracted}
                 subtraction of the post-bleach to the pre-bleach
                 image, gives a clear indication of the bleach spot,
                 as well as faint signal for the nuclear region due to
                 unintentional photobleaching;
                 \subcaptionref{subfig:software:frap-roi:prebleach}
                 perimeter of the automatically identified ROIs
                 superimposed on the pre-bleach image: cell nuclei,
                 bleach spot, and background region.  }
               \label{fig:software:frap-roi}
\end{figure}


With all the steps automated, we created a single program written in Octave
that we named \command{frapinator} to perform all the
analysis for any number of images.  All options were made
available as command line options.
To quickly filter out any faulty analyses, two multi-panel
images are created that provide a visual log.  One image shows the
automatically identified regions \frefp{fig:software:frap-roi} and
another shows the recovery curves, intermediary analysis, and best fits
\frefp{fig:software:frapinator}.

\begin{sidewaysfigure}
  \includegraphics[trim=7cm 3cm 5cm 2.5cm, clip=true, width=\textwidth]%
                  {results/frapinator.png}
  \captionIntro{Frapinator visual log files for batch processing}
               {Each FRAP experiment generates a log file with 6
                 different plots displaying the analysed values and
                 the fitting to different models.  In conjunction with
                 the images in \fref{fig:software:frap-roi} this
                 provides an overview of the entire analysis
                 process.  The top left plot displays the raw
                 intensity for the background, bleach spot, and
                 nucleus intensity over the duration of the FRAP
                 experiment. This is followed by the normalised
                 intensity for the bleach spot which is actually used
                 for the fitting. The top right displays the intensity
                 profile for the bleach spot, and its fit to a radial
                 profile model. The three bottom panels display the
                 data fitted to three different models: pure diffusion
                 which has no terms for binding constants; full model
                 with a fixed diffusion rate; and full model with all
                 the 3 terms.  }
               \label{fig:software:frapinator}
\end{sidewaysfigure}

We packaged the \command{frapinator} program and all the FRAP analysis
functions into an Octave FRAP package and made it available
online at \url{https://github.com/carandraug/octave-frap}.

\section{BioPerl}

The BioPerl project is an international association of developers of
free Perl software for bioinformatics, genomics, and life science
\citep{bioperl}.  It has created the BioPerl distribution of Perl
modules which contains almost 800 modules for management and manipulation
of biological data as well as programmatic access to databases such as GenBank
and SwissProt, and to bioinformatics tools such as ClustalW and Blast+.

%% $ find bioperl-live/Bio/ -name '*.pm' -type f | wc -l
%% 791
%%
%% This number no longer includes the modules already moved out of
%% BioPerl and into their own distributions.

%% Technically, the first one to be moved out of BioPerl was
%% Bio-Graphics and that was in 2009.  However, that was an
%% independent event, not part of a concerted effort of reorganising
%% BioPerl.  That only started in 2011 (see Changes file in
%% bioperl-live).
The large number of modules in BioPerl became a maintenance problem
so in 2011, a new project was initiated to split BioPerl into
more manageable distributions such as Bio-Biblio, Bio-FeatureIO, and
Bio-Coordinate.

\subsection{Dist::Zilla and Pod::Weaver}

To reduce existing code, prevent duplication,
and to make new releases
a easier, Dist::Zilla and Pod::Weaver were adopted for
the new distributions.

Dist::Zilla is a program facilitating the writing, packaging, management,
and release of free software for libraries that are written in the Perl
programming language and released to the Comprehensive Perl Archive
Network (CPAN) repository.  It comes with a series of plugins to automate the
release process such as the addition of copyright notices, discovery
of dependencies, and uploading to CPAN.

Pod::Weaver is a program to create documents in Plain Old
Documentation (POD) format, a format mainly used to write
documentation incorporated inline within Perl modules.
Pod::Weaver includes a Dist::Zilla plugin
so that most standard generic POD content is
generated automatically as part of the release process.

As part of the restructure of the BioPerl distribution, we configured
the BioPerl Dist::Zilla plugin bundle, and created two new Pod::Weaver
section plugins, GenerateSection and Legal::Complicated.

GenerateSection creates POD sections based on templates.  It is
used in BioPerl to generate the support section of documentation
of individual modules
with distribution specific details such as links to the online
repository.

Legal::Complicated creates a POD section for copyright details
based on comments in individual modules.  This is useful because while
BioPerl is free
software, individual modules may be released under different free
software licenses, and each has their own author who is often not
the copyright holder.

Both new Pod::Weaver plugins and the BioPerl PluginBundle are available
on CPAN since 2013.  They were used for our contributions to the
Bio-EUtilities package.

\subsection{Bio-EUtilities}

For the analysis of the canonical histone gene
family \Crefp{ch:histone-catalogue} we required a tool to
automate the search of histone genes and download associated sequences.
We used the NCBI Gene database for searches and
created a new program \command{bp\_genbank\_ref\_extractor}
within the Bio-EUtilities package of BioPerl.

Bio-EUtilities is part of the BioPerl project and provides a Perl
interface to NCBI's Entrez Programming Utilities (E-Utilities).
Entrez is a federated search engine for multiple databases of
biomedical data including Gene.  Entrez has an
interactive interface at \url{https://www.ncbi.nlm.nih.gov/} while
E-Utilities provides an equivalent
programming interface for queries using a fixed
URL syntax.

Gene is a public database hosted at the National Center for
Biotechnology Information (NCBI) which maps known or predicted genes
to other entries in the NCBI Reference Sequence (RefSeq).  Therefore, Gene
links to the Genome, Nucleotide, and Protein databases \citep{gene-database}.

The program \command{bp\_genbank\_ref\_extractor} was
created to take a query to the Entrez Gene database as input and
to downloads all genomic, transcript, and protein sequences as well
as a CSV file with chromosome coordinates, names, and identifiers of
returned genes as output.
It has several options such as download of flanking sequences, different
output formats, choice of genome assembly, and skipping of non coding
genes.

\command{bp\_genbank\_ref\_extractor}
is provided with an extensive manual covering all options
and examples \Arefp{app:pod-doc}.

\command{bp\_genbank\_ref\_extractor} was released with
Bio-EUtilities version 1.73.

\subsection{Debian packaging}

Debian is a computer operating system
composed entirely of free software and is one of the earliest GNU/Linux
distributions.

Debian is package based like all modern free operating systems.
This means that it is made out of multiple components known as
packages.  For example, in Debian there are packages for the Linux
kernel, the Perl programming language, and Dist::Zilla.  Packages are
managed by a package management system which handles their
installation, configuration, and removal to simplify
a multiplicity of small steps that would otherwise
have to be handled manually by the user.

%% Debian Jessie has 21024 source packages.  The binary number of
%% packages would be much higher but while that number is what is
%% usually used when making a point for Debian's high number of
%% packages, it is misleading.  The point is how much of upstream
%% projects are packaged, and that is the number of source packages.

Debian is a widely used GNU/Linux
distribution with more than 21000 packages and a large
number of derivative distributions.  These new distributions
inherit the base of their packages from Debian, and some like Ubuntu
and Knoppix are in turn the parents of their own derivatives.  By packaging
for Debian, a packager effectively prepares packages for the whole family of
Debian based distributions.

While Debian had a package for the main BioPerl distribution, it did
not have one for Bio-EUtilities.  We packaged Bio-EUtilities for
Debian with the aim of making it easier for
others to reproduce our results.
Similarly, to make it easier for prospective BioPerl
developers, we packaged all the Dist::Zilla plugins required to produce
new BioPerl releases as well
as all the module distributions required by them
\trefp{tab:software:debian-packages}.

\begin{table}
  \captionIntro{Perl module distributions packaged for Debian}{}
  \label{tab:software:debian-packages}
  %% This one line descriptions were retrieved from Debian description
  %% which we wrote (except the one for Bio-EUtilities).
  \begin{description}
  \item[Bio-EUtilities] \hfill \\
    Webagent which interacts with and retrieves data from NCBI's E-Utils.
  \item[Config-MVP-Slicer] \hfill \\
    Module to extract embedded plugin config from parent config.
  \item[Dist-Zilla-Config-Slicer] \hfill \\
    Config::MVP::Slicer customized for Dist::Zilla.
  \item[Dist-Zilla-Plugin-AutoMetaResources] \hfill \\
    Dist::Zilla plugin to ease filling \command{resources} metadata.
  \item[Dist-Zilla-Plugin-MojibakeTests] \hfill \\
    Dist::Zilla plugin that provides author tests for source encoding.
  \item[Dist-Zilla-Plugin-ReadmeFromPod] \hfill \\
    Dist::Zilla plugin to generate a README from POD.
  \item[Dist-Zilla-Plugin-Test-Compile] \hfill \\
    Common tests to check syntax of Perl modules, using only core modules.
  \item[Dist-Zilla-Role-PluginBundle-PluginRemover] \hfill \\
    Dist::Zilla plugin to add \command{-remove} functionality to a bundle.
  \item[MooseX-Types-Email] \hfill \\
    Email address validation type constraints for Moose.
  \item[Pod-Weaver-Plugin-EnsureUniqueSections] \hfill \\
    Pod::Weaver plugin to check for duplicate POD section headers.
  \item[Pod-Weaver-Section-Contributors] \hfill \\
    Pod::Weaver plugin for a section listing contributors.
  \item[Pod-Weaver-Section-GenerateSection] \hfill \\
    Pod::Weaver plugin to add POD sections from a template text.
  \item[Pod-Weaver-Section-Legal-Complicated] \hfill \\
    Pod::Weaver plugin for per module authors, copyright holders, and license.
  \item[Test-Mojibake] \hfill \\
    Module to check source for encoding misbehaviour.
  \end{description}
\end{table}


\section{Build systems for reproducible research}

Even if the original data is available for a computational biology
investigation such as microscopy image or genome sequence analysis
and the runtime environment can be duplicated, reproducing results
is dependent on invoking the same commands and same
options in the same order as the original analysis.
The necessary information to achieve this is often undocumented
and difficult to reconstruct.

A build system is a software tool that automates the process of
performing a complex series of steps for the generation of an artefact.
It is mainly used in software engineering to automate software
compilation and packaging
but the process of maintaining software has many parallels with
maintaining reproducible research projects in computational biology
so the same tools
can be used.  For example,
in a software compilation project, object code is built from the
source code whereas in a research project, figures, tables, and values
are built from raw
data.  Likewise,
in a software project, an executable program is built from multiple
object code whereas in a research project a manuscript is built from the
figures, tables, and values.

%% On ReDocs there is a set of rules to prepare figures and run the
%% analysis.  What he proposed was a set of names to generate such
%% figures, run analysis, skip analysis that would take too long, and
%% remove intermediary files.  This is similar to what GNUs coding
%% standarda mandates via automake, all Makefiles should support
%% install, all, help, check, so users know immediately what each one
%% does.
Claerbout and colleagues \citep{Claerbout2000} proposed this
parallel between software engineering and research projects, and
coined the term ``reproducible research''.  They created a standard
build system for the generation of figures and manuscript from author
data and analysis software as an extension to GNU
Make, one the most common build system.  Following on from this,
Madagascar \citep{madagascar-scons} was developed as a
software solution specialised
for reproducible computational experiments, based on the software
build system SCons.

The SCons (from Software Construction) build
system \citep{SCons2005} was designed to be a
replacement of Make and resembles Make in concept.  However, it has
the advantage of being configured using Python which is a modern
programming language often praised for its readability.

%% Other people may mention other SCons advantages but I disagree:
%% 1.SCons has builtin configure and dependency analysis.  Well, that
%%   is bullshit.  First, it doesn't work properly.  Configure support
%%   was a second thought and is very much incomplete.  Second,
%%   scanner is really slow.  And while Make itself really does not
%%   have them, it is meant to be used as part of Autotools where
%%   those jobs are done by autoconf and automake.
%% 2. SCons uses a liberal licence.  Well, I prefer copyleft and
%%    anyway that's the build system.
%%
%% One advantage that I could agree is default to md5sums instead of
%% timestamps but then I would have to explain what md5 and a checksum
%% is.  It also has builtin support for latex but that's really easy
%% to do in GNU Make.

For the work in \Cref{ch:histone-catalogue} we made extensive use of
the Perl programming language which is not supported by default in
SCons but is commonly used in bioinformatics.
To simplify the use of SCons for our project we created
a SCons perl tool which adds automatic prerequesite
scanning, perl configuration options, and multiple functions for using
Perl scripts.

The SCons perl tool is available at
\url{https://bitbucket.org/carandraug/scons-perl5} under a free
software licence.

%% automatic prerequesite scanning -> Scanner
%% configuration options -> Variables
%% multiple functions -> all the perl builders.

\section{Discussion}

Reproducible research has two principal motivations.
One reason is to allow other
researchers to reliably perform the same analysis.  This can be on the same
dataset as the original research in order to identify possible errors in the
methodology and reflects the essential replicability requirement of all science.
When associated with the freedom to inspect the
source code, reproducible research enables anyone
to check for errors directly in the
code.  All software used in this thesis is free and open source so any
part of it can be interrogated and replicated.

The second reason, and the one of most important concerns of this chapter, is to
enable others to use the tools and insights
developed for new research directions.  This
necessitates a particular approach to the development of scientific
software, requiring more insight in the software design and
engineering to ensure its usefulness.

It is not sufficient to develop software that solves our problem
within the constraints of our data
because the software must also be configurable and
generalised so that it can fit the needs of other users.  For example, the
Pod::Weaver plugins we created to solve the problem of making BioPerl
releases self-documenting was implemented
with extra capabilities that were not needed
for the project.
The code could also have been implemented
directly in the BioPerl PluginBundle but having
them as separate individual plugins enables other Perl software projects to use
them.

As free software, there is no sales or reliable download data, so
we do not know exactly how many people use
our work since we mainly hear when parts don't work.  We do
know people are using it because they have reported issues and even suggested
improvements to us.  This happened for the Pod::Weaver plugins
when users with newer versions of Perl
reported issues before we had tested them ourselves.

Users reporting issues are an important signal of interest so
the more users we have the more issues are reported to us.
Our work with Octave is likely to have had the most impact since
issues are reported or discussed for the Octave Forge Image package
several times a month.  However, our work there has
become positively entangled with that of other contributors
so we cannot measure the exact impact of single contributors alone.
This means that even if we have the information on how many people use Octave,
and how many use the Image package, it is impossible to measure
how much use is being made of the specific
functions we improved.

We recognise that full fledged programs impose limitations even though
they are popular with more users who want ``turn key'' solutions.
A program is easier to make use of by novice users
but the availability of the same functionality as a software library allows
reuse by other software developers, and the two are not mutually exclusive.
This can be addressed by coding with the potential to be useful for
others by writing with modularity in mind so that any part can be used in
other programs.  For example, when writing \command{frapinator}
to perform our analysis of FRAP data, we wrote in such a way that the
program is a shell around our Octave FRAP
package.  \command{frapinator} simply reads the options and then calls
a series of a functions from the FRAP package.
This allows other
researchers to reuse the parts they need.  For example, image analysts
can utilise
the functions to identify the bleach spot but implement alternative FRAP
models, possibly from another such library.

Maintaining the balance of flexibility and usability requires
additional effort.
For example, our choice to work with the Debian base to package software we
needed for our projects stemmed from the difficulty members of our group
had with installing some of the required dependencies.  We could have
installed it on their behalf but that wouldn't benefit other
researchers in the same position.
The situation could also reoccur each time a user got a new
machine or update.

Another alternative would be to include
dependencies within our software.  However, this has its own risks by
making the dependencies opaque.  For example, part way through our project
NCBI moved their E-Utilities services to an encrypted network protocol which had
been quickly addressed in Bio-Eutilities
but would have necessitated updating our
entire histone catalogue \Crefp{ch:histone-catalogue} if we had
included an outdated version of Bio-EUtilities directly with it.

\section{Conclusion}

The various software tools reported in this chapter were developed
because they were needed for our research.  Reports from users suggest
that they have been useful for others.  Furthermore, the software has
been developed in a modular way to facilitate reuse, but also made
available with packaging to enhance general uptake.
