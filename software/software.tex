Today's biology research is mainly digital.  The generated data is so
complicated that its analysis is done by software, and computer data
acquisition and analysis is an integral part of a researchers work.
Such work is then published, but it is only useful to the scientific
community if it can be reproduced by their colleagues and further
research can be performed on top of it.  Even if the software used is
carefully described in the methods, modern programs are so
complicated, and analysis is dependent on so many variables, that
there's no hope for other scientists to reimplement the software.  This
makes the original software used in research, a prerequesite for
reproducibility.

But sharing the code is not enough.  Programs that do the analaysis
sit on top of several other components.  Specific versions of a
library are used, on top of a specific programming language, running
in a specific operating system on a hardware with a specification
specification.  Reproducing this runtime environment is not trivial.

Following free software, came the ideals of open and collaborative
software development.  In the seminal assay ``The Cathedral and the
Bazaar'' by \citet{raymond1999-cathedral-and-bazaar}, compares a system
where software development happens in the public.  Anyone can see it
and contribute to it.

In free software, the source code for a program is available on the
internet for anyone.  The bazaar takes this further by having every
single change open on the internet in real time, before a new version
of the software is released.  This invites users to contribute.
Discussion about development happens in public forums, more inviting
users to contribute.  The ideal is that any user can become a
developer, they simply need to send their contributions to this public
forum.  Users are motivated to do this because they ``are scratching
their own itch'', they have a personal interest on enabling the
program to support a specific feature that matters to them.  And they
are motivated to contribute to the project because it is less work
than writing their own project from scratch just to have the feature
they lack.

Free software projects are nowadays all virually developed in this way
and so are many academic free software
projects \citep{schindelin2012fiji, bioperl}.. We have faced such
issues during our own research and had many ``itches'' to scratch.
Using only free software we were able to scratch them and got involved
in several of this projects free software projects that go beyond the
boundaries of the typical research group.  While this advances allowed
us to perform the acquisition of our data, we expect that this form
will enable other researchers to advance their own research.

\section{GNU Octave}

The GNU Octave programming language was used extensively for the analysis
of microscope images throughout this work.  Several features of the
language such as its handling of multi-dimensional arrays, interactive
user interface, and extensive image processing functions, as well as
its supportive community, make it an attractive choice
for quantitative microscopy.

%% All values are multi-dimensional arrays does not include objects
%% wrapped from other languages such as java objects (and I guess it
%% will be the same for python when the pytave project is finished).
GNU Octave is an high level array programming
language where operations are generalised to multidimensional arrays
and is primarily intended for numerical computations \citep{octave}.
In Octave, all values are multidimensional arrays and this provides
an abstraction layer that is useful when writing code for an arbitrary
number of dimensions.

%% Doesn't seem to be any scientific study about REPL as an advantage
%% for prototyping.  Similarly, none about being better for
%% exploratory data analysis but I can't imagine anyone disagreeing
%% with it.
Octave also has a Read--Eval--Print Loop (REPL), or interactive top
level user interface, like IPython and Unix shells.  This reduces the
feedback time and provides an efficient
environment for exploratory data analysis.

%% No citation for this, only the anecdotal evidence of being the
%% project leader of octave Forge for 6 years.
The project has an active community
mainly composed of
scientists and engineers who provide a large support group of
specialists in numerical computations.

The separate Octave Forge project hosts a vast number of
packages that extend Octave into specific applications such as control
systems \citep{octave-control}, time-frequency analysis
\citep{octave-ltfat}, or level sets \citep{octave-level-set}.
It provides a collaborative environment for development of Octave
packages and another nexus of the Octave community.

Both GNU Octave and Octave Forge packages are free software which allows
the study, modification, and distribution of modified source.
We made extensive
use of this feature and have contributed to improving Octave and its
packages for the needs of quantitative microscopy.

Finally, Octave comes with basic support for image processing and the
Octave Forge Image package extends this with a number of functions for
image processing such as geometric transformations, mathematical
morphology, image registration, and noise reduction.

%% These were actually lsm files but those are actually TIFFs.
While Octave is well suited to image
processing, we identified several problems related to
large image size or number of dimensions.

Our microscopy images
were several megabytes in size.  For example, the single cell
FRAP experiments for histone dynamics generating TIFF files of 449
These images had a field of view of 300
by 300 pixels and 2500 time frames
on acquisition by an 8-bit camera.
Such files sizes are typical in the microscopy field.

Biological microscope images have a varying number of dimensions.
In addition to the 2 dimensional plane of a standard image, microscope
outputs typically include any combination of $z$ dimension along
the optical axis for a volume image; time dimension for time-lapse
experiments; and wavelength for multi-channel experiments.  Recent microscopy
techniques generate images with an even higher number of dimensions
such as angle, phase, and lifetime.  This varying dimensionality adds
complexity and makes it challenging to write generalized subroutines for
microscope image analysis.

\subsection{Reading and Writing of Image Files}

%% Number of 88 major image formats taken from GraphicsMagick
%% documentation on February 2017.
Octave uses the GraphicsMagick C++ library for the reading and writing
of image files which provides a common interface
to a varied collection of
image format specific libraries covering almost
90 major image formats.  In Octave, this functionality is
provided via the functions \command{imfinfo}, \command{imread}, and
\command{imwrite}.

%% The importance of reading all planes in a single call to imread is
%% of performance for TIFF.  This is because of the structure of TIFFs
%% where reading plane N means parsing all the IFDs before that.
These three functions were rewritten with the aim of achieving
reduced memory usage, increased performance, an improved
interface for multidimensional images, and new image types.
New options were added to read and write a series of image planes
in a single function call, to read specific regions of interest in
individual planes, to read and write images
with floating point precision, and to append additional planes to existing image
files.

As part of this rewrite, new features were introduced such as
support for the CMYK colour model, EXIF
and GPS metadata, reading and writing of animations in GIF images, and
control over the image compression type,
while existing features were improved such as
support for transparency and
indexed images.

%% We can't use the term image IO functions because then we would have
%% to explain what IO is.  For the same reason, we can't say ``hooks''.
To support this development, we created a system that allows changing
how the \command{imread}, \command{imwrite}, and \command{imfinfo}
functions behave for each file format.
This enables Octave extensions to add support for new image file
formats or to improve access to the existing formats
without addition of new format specific functions.
For example, it is now possible for packages to enable reading of
microscope specific metadata from \command{imfinfo}.
This system is available via the function \command{imformats}.

Changes were required in other Octave functions related to images
including those involved in conversion between color
models, grayscale images, and indexed images to support integer and
floating point data types, and multiple dimensions
\trefp{tab:software:octave-core-functions}.

All changes were released with Octave version 3.8.0.

\begin{table}
  %% This table includes the major things we contributed to Octave
  %% core.  It doesn't mention a version since it's work I contributed
  %% between Octave 3.6 and 4.2.  Some of the changes even span
  %% multiple versions.
  %% It does not include minor bug fixes or trivial bug fixes.  Also
  %% does not include major changes we did to the internals which only
  %% reduced code duplication.
  \captionIntro{New or improved functions in GNU Octave}{}
  \label{tab:software:octave-core-functions}
  \begin{multicols}{3}
    \begin{itemize}[label={}]
      \foreach \function in {
        bitcmp,
        bzip2,
        cubehelix,
        fliplr,
        flipud,
        flip,
        frame2im,
        gallery,
        gray2ind,
        gzip,
        hsv2rgb,
        im2double,
        im2frame,
        imfinfo,
        imformats,
        imread,
        imwrite,
        ind2gray,
        ind2rgb,
        inputParser,
        ntsc2rgb,
        pkg,
        psi,
        rectint,
        rgb2hsv,
        rgb2ind,
        rgb2ntsc,
        rot90,
        validateattributes}
      { \item \command{\function} }
    \end{itemize}
  \end{multicols}
\end{table}

\subsection{Bio-Formats and Octave Java interface}

%% We kind of managed to read the pixel data.  This also required
%% knowing that LSM interleaves the actual pixel data with thumbnails
%% which need to be skipped.
While GraphicsMagick provides support to read many image formats, its
support for scientific microscope image formats is limited.
For example, Zeiss confocal microscopes saves
images in the LSM file format which is a proprietary extension of TIFF.
Although GraphicsMagick reads LSM pixel data,
the file metadata such as pixel size, time interval, or region of
bleaching event cannot be retrieved.

Bio-Formats is a free software library for reading and writing image
data with a strong focus on microscopy image file formats
\citep{bioformats}.  It is written in the Java programming language and
used by other programs in the field of microscope image analysis such
as CellProfiler \citep{cellprofiler}, ImageJ \citep{imagej2}, and OMERO
\citep{omero}.

%% Before version 3.8.0, there was a separate Octave Forge package
%% that added the java interface.  Actually, Octave version 3.8.0
%% pretty much only merged the java package into it.
Octave has a native interface to the Java programming language
that should enable easy integration with
Bio-Formats, and Bio-Formats has a Matlab toolbox that should be
compatible with Octave.
We identified a series of problems in the Octave interface to Java that
could be solved either by improving Octave or Bio-Formats.

%% Writing this paragraph without mentioning the words data type,
%% primitive, wrapper, and objects is though.  And on top of that,
%% writing it without sounding dumb is just impossible.
In Octave, we rewrote the handling of values returned from Java.
Of special importance for BioFormats integration was the automatic conversion of
Java arrays which are used to return pixel data.
Support for conversion of multidimensional Java arrays was not implemented
since we did not require it and this remains open as a future project.

In Bio-Formats, we modified the Matlab toolbox so that it is
both Octave and Matlab compatible.
This change simplifies the packaging of
Bio-Formats for Matlab and Octave making them
effectively the same code.
Finally, we automated the creation of Octave packages in Bio-Formats
so that they be made as part of standard Bio-Formats releases.

All changes were released with Octave since version 4.0.0 and
Bio-Formats since version 5.1.2

\section{Octave Forge Image package}

The Octave programming language is primarily intended for numerical
computations and provides a syntax and set of functions that is particularly
convenient for solving linear algebra and differential equations.  It
also includes a collection of functions for the handling of image data
but these are focused on the reading and writing of image files,
conversion between colour models, and graphical display.

The Octave Forge Image package supplements Octave with a wide range of
image analysis specialised functions.  While Octave syntax is
identical for any number of dimensions and data type, we found that
many of the Image package functions were either limited to two
dimensional images or inefficient for the large images such as those used in
microscopy.  We began a project with the purpose of eliminating any
limitation on the number of dimensions in the Image package
\trefp{tab:software:octave-image-functions}.  This has seen multiple
releases, starting in version 2.0.0 and continuing to the current
version 2.6.1.
%% Other issues are when functions just treated higher number of
%% dimensions as a long 2D image and introduced artifacts on image
%% borders.  Or they looped over each pixel making it terribly slow
%% even on 2d images.
%%
%% The first time the project to support ND images was made official
%% was for the summer of 2013 (my GSOC) but even on that proposal I
%% mentioned work already made.

\begin{table}
  \captionIntro{New or improved functions in the Octave Forge Image
    package}{}
  \label{tab:software:octave-image-functions}
  \begin{multicols}{3}
    \begin{itemize}[label={}]
      \foreach \function in {
        bestblk,
        bwareafilt,
        bwareaopen,
        bwconncomp,
        bwdist,
        bwlabel,
        bwlabeln,
        bwmorph,
        bwperim,
        bwpropfilt,
        checkerboard,
        col2im,
        colfilt,
        conndef,
        edgetaper,
        fftconv2,
        fftconvn,
        grayslice,
        graythresh,
        im2col,
        imabsdiff,
        imadjust,
        imattributes,
        imbothat,
        imclearborder,
        imclose,
        imcomplement,
        imcrop,
        imdilate,
        imerode,
        imfill,
        imgetfile,
        imhist,
        imlincomb,
        immse,
        imopen,
        impixel,
        impyramid,
        imquantize,
        imreconstruct,
        imregionalmax,
        imregionalmin,
        imresize,
        imrotate,
        imtophat,
        intlut,
        iptcheckconn,
        label2rgb,
        labelmatrix,
        mat2gray,
        mmgradm,
        montage,
        nlfilter,
        normxcorr2,
        ordfiltn,
        otf2psf,
        padarray,
        psf2otf,
        psnr,
        regionprops,
        rgb2ycbcr,
        strel,
        stretchlim,
        subimage,
        tiff\_tag\_read,
        watershed,
        wavelength2rgb,
        ycbcr2rgb}
      { \item \command{\function} }
  \end{itemize}
  \end{multicols}
\end{table}

\subsection{Image Thresholding Algorithms}

Image thresholding is a method for image segmentation where the image
pixels are separated into classes based on their intensity values.
The simplest of thresholding methods uses a single fixed value and
separates an image into background and foreground which are then
represented in a binary black or white image.  This is
particularly useful for fluorescence microscopy images where objects
of interest show bright high intensity values against a dark
background of low intensity values.

Multiple threshold algorithms exist to automate the choice of a
threshold value, the most common of which is based on the analysis
of an image histogram \citep{analysis-hist-threshold}.  Otsu's method
\citep{otsu-threshold} is among the most widely used
and is available in the function \command{graythresh} in the Image
package.  We rewrote this function to handle histograms of arbitrary
length and vectorised it for performance with additional computation of a
``goodness'' measure of threshold value optimality.
In addition, an unpublished collection of histogram
threshold algorithms by Antti Niemistö (personal communication),
making them available as an
option for \command{graythresh} in Octave.  Finally, the rewrite
of \command{graythresh} added the
option of using an histogram as input instead of an image, enabling
the possibility of histogram processing as a preparatory step.

%% We have no reference for Antti Niemistö work.  It was never
%% published and only available on his University webpage at
%% http://www.cs.tut.fi/~ant/histthresh/ which is no longer available.
%% He has several other publications but none about this topic.
%% Here's one: A. Niemistö, V. Dunmire, O. Yli-Harja, W. Zhang, and
%% I. Shmulevich, "Robust quantification of in vitro angiogenesis
%% through image analysis," IEEE Transactions on Medical Imaging,
%% vol. 24, no. 4, pp. 549-553, April 2005.
%%
%% The only copies of his work that I know of are on my github account
%% (which I have since modified), and on https://archive.org/web

All these changes were released in the Octave Forge Image
package version 2.0.0.

\subsection{Mathematical Morphology}

%% This is not the complete story.  When we started, imdilate and
%% imerode had some support for ND images.  I can't quite remember
%% what was missing but I do remember making a table at the end with
%% performance comparison, and some comparisons couldn't be made
%% because they were missing before.  Looking at the source for 1.0.15
%% (before I started this), I can see that imerode and imdilate called
%% ordfiltn for grayscale images which was bound to be quite slow, and
%% filter2 for binary images which would limit to 2d images.
Mathematical morphology is the analysis of spatial
structures which provides a powerful image analysis technique based on
the shape and form of objects.  This is achieved by probing an image
with a known shape known as the Structuring Element (SE), and filtering the
image based on whether the SE fits at each location
within the image.

The fundamental operations of mathematical morphology are named dilation and
erosion.  These are available in the Image package through the
\command{imdilate} and \command{imerode} functions.  More complex
morphological operations are built on top of these two operations.  For
example, the morphological top-hat transform corresponds to the difference
between an image and its morphological opening, which in turn
corresponds to an erosion followed by a dilation.

As the two fundamental operations, dilation and erosion are ideal targets for
improvement since any performance increase or
support for new image types will be propagated to
higher level morphology functions.

%% The original imdilate and imerode used __spatial_filtering__ with
%% the ordefiltn option.  We first optimised this by using convn which
%% made it a lot faster but only for binary images.  We then modified
%% __spatial_filtering__ so that it was faster in grayscale images
%% too.  We ended up writing a specialised function for dilation and
%% erosion.

%% My notes say that perfomance gains were between 1.5-30x for erosion
%% and dilation, and 1/5 to 2/5 faster for __spatial_filtering__.  At
%% least the erosion and dilation were on my old blog which got lost
%% after the server was damaged.
%% SE specialisation == SE decomposition
\command{imdilate} and \command{imerode} used the general purpose
\command{\_\_spatial\_filtering\_\_} function of the Image package.
We rewrote the functions aimed at morphology operations with
specialisations for different data types and SE.
A new \command{strel} class was created for the SE specialisation which
supported both flat and non-flat SE.

These changes were released with
the Octave Forge Image package version 2.2.0.

%% Not mentioning the rewrite of __spatial_filtering__ because the
%% only reason to do so was performance.  While my notes mention nice
%% values, I don't have the time to make proper performance comparison
%% so better not even talk about it.

\subsection{Image Regions of Interest}

The function \command{regioprops} is used to measure different
properties in image regions of interest such as a region centroid,
area, or eccentricity.  However, this function was inappropriate for
the measurement of multiple regions and properties because the whole
image was analysed independently for each region and property.  We rewrote
this function for improved efficiency so that
computation of area and intensity weighted
centroid of 2000 regions which previously took over 12 hours on a
standard desktop computer was finish in under 3 minutes.
This improvement was released with the Image package version 2.6.0.
%% We say over 12 hours but actually they never finished.  Not having
%% finished after so many hours (overnight) was absolutely not
%% acceptable.  Estimates say that it would actually take over ~200
%% hours to finish.
%% The images I used for testing were Ezequiel's histone modification
%% markers, the images were 512x512x30 with slightly more than 2000
%% very small objects.

%% Actually there was a version of bwconncomp before but it didn't do
%% this, it only identified the boundaries of each object.  So while
%% technically, this function name existed, it was something else.
%% Our first attempt made use bwlabeln but then it had to undo the
%% labelling which is inefficient.  That was released with version
%% 2.2.0.  The writing of it as a function performing flood-fill was
%% done later and released with image 2.4.0.
We also wrote the function \command{bwconncomp} to perform the
identification of image regions with an arbitrary number of
dimensions.
\command{bwconncomp} is used internally by \command{regionprops} and
creates an array of indices for the image regions as an
alternative to labelled images which reduces memory usage.
This was released with the Image
package version 2.4.0.

%% Other Octave that I am not mentioning:
%%   * move all XMatrix as subclasses of XNDarray to reduce code
%%     duplication.
%%  * liboctave better integration with STL
%%  * templates for svd instead of duplicated classes
%%  * cleanup of methods (capacity, length, nzmax)
%%  * making Octave quiet when calling scripts of eval.
%%  * zero length dimensions on broadcasting
%%  * accumarray, zeros, ones, etc with empty lists
%%  * rewrite 'pkg build' for distribution of binary packages.
%%  * miscellaneous package: textable and units
%%  * signal package: xcorr2 coeff option
%%  * statistics: hist3, grp2idx, squareform, and binostat
%%  * mapping: a bunch of low hanging fruits
%%  * dicom: add support for any version of gdcm.
%%
%%  * all the work on the zenity package.  Maybe I will still write
%%    about it...
%%  * normxcorr2 in the image package.

\section{Octave FRAP package}
\label{sec:software:octave-frap}

For the FRAP analysis of histones
in \Cref{ch:kill-frap} we required tools to estimate
binding constants from FRAP recovery data.  We obtained the code for a
previously reported circle FRAP model \citep{mcnally-frap-code} from
personal communication with the authors under the GNU General Public
Licence (GPL) version~3 or later.  This model includes multiple
parameters and considers the profile of the photobleach, correction
for observational photobleaching, finite size of the nucleus, and
fitting to both a pure-diffusion model and a full model with binding
states.

This code was written in the Matlab programming language so was
easily ported to Octave.  The main difference was the replacement of
the nonlinear fitting from \command{nlinfit} with \command{leasqr}
from the Octave Forge Optim package since both perform the same
Levenberg--Marquardt nonlinear least squares algorithm.

Identification of the bleach spot, nucleus, and background regions are
required for the circle FRAP model.  The bleach spot measures
intensity recovery and also models the photobleach profile since it
takes into account a non-uniform spatial distribution of the bleached
spot.  The nucleus region defines the finite sized nucleus and takes
into account the fluorescence loss due to observational
photobleaching.  A small region outside the nucleus is used for
background correction.

We automated the identification of these regions to facilitate
batch processing.  The bleach spot was
identified from the difference between the post and pre-bleach frames.
Individual nuclei were then segmented after automatic thresholding with Otsu's
method.  A background region was identified as the rectangle of a
fixed size with lowest average intensity in the image.

\begin{figure}
  \centering
  \subbottom[pre-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-prebleach.png}
                    \label{subfig:software:frap-roi:prebleach}
  }
  \hfill
  \subbottom[post-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-postbleach.png}
                    \label{subfig:software:frap-roi:postbleach}
  }
  \subbottom[pre-bleach $-$ post-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-subtracted.png}
                    \label{subfig:software:frap-roi:subtracted}
  }
  \hfill
  \subbottom[Identified ROIs]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-selected.png}
                    \label{subfig:software:frap-roi:selected}
  }
  %% These images are not the visual logs because then we would not be
  %% able to have them as separate subfigures or add a scalebar.
  \captionIntro{Automatic selection of regions for FRAP analysis}
               {HeLa stable cell line expressing the H4~R45H mutant
                 tagged with YFP were imaged every \SI{30}{\ms} in a
                 confocal microscope. A circular shape is used for
                 photobleaching after 100~frames.
                 \subcaptionref{subfig:software:frap-roi:prebleach}
                 averaging of 50~pre-bleach images removes most of the
                 noise, allowing for a better refined ROI;
                 \subcaptionref{subfig:software:frap-roi:postbleach}
                 average of 5 post-bleach images;
                 \subcaptionref{subfig:software:frap-roi:subtracted}
                 subtraction of the post-bleach to the pre-bleach
                 image, gives a clear indication of the bleach spot,
                 as well as faint signal for the nuclear region due to
                 unintentional photobleaching;
                 \subcaptionref{subfig:software:frap-roi:prebleach}
                 perimeter of the automatically identified ROIs
                 superimposed on the pre-bleach image: cell nuclei,
                 bleach spot, and background region.  }
               \label{fig:software:frap-roi}
\end{figure}


With all the steps automated, we created a single program in Octave
named \command{frapinator} that would perform all the
analysis for any number of images.  All options were made
available as command line options.
To quickly filter out any faulty analyses,
two images are created that provide a visual log.  One image shows the
automatically identified regions \frefp{fig:software:frap-roi} and
another shows the recovery curves, intermediary analysis, and best fits
\frefp{fig:software:frapinator}.

\begin{sidewaysfigure}
  \includegraphics[width=\textwidth]{results/frapinator.png}
  \captionIntro{Frapinator visual log files for batch processing}
               {Each FRAP experiment generates a log file with 6
                 different plots displaying the analysed values and
                 the fitting to different models.  In conjunction with
                 the images in \fref{fig:software:frap-roi} this
                 provides an overview of the entire analysis
                 process.  The top left plot displays the raw
                 intensity for the background, bleach spot, and
                 nucleus intensity over the duration of the FRAP
                 experiment. This is followed by the normalized
                 intensity for the bleach spot which is actually used
                 for the fitting. The top right displays the intensity
                 profile for the bleach spot, and its fit to a radial
                 profile model. The three bottom panels display the
                 data fitted to three different models: pure diffusion
                 which has no terms for binding constants; full model
                 with a fixed diffusion rate; and full model with all
                 the 3 terms.  }
               \label{fig:software:frapinator}
\end{sidewaysfigure}

We packaged the \command{frapinator} program and all the FRAP analysis
functions into an Octave FRAP package and made it available
online at \url{https://github.com/carandraug/octave-frap}.

\section{BioPerl}

The BioPerl project is an international association of developers of
free Perl software for bioinformatics, genomics, and life science
\citep{bioperl}.  It has created the BioPerl distribution of Perl
modules which has almost 800 modules for management and manipulation
of biological data as well as programmatic access to databases such as GenBank
and SwissProt, and to bioinformatics tools such as ClustalW and Blast+.

%% $ find bioperl-live/Bio/ -name '*.pm' -type f | wc -l
%% 791
%%
%% This number no longer includes the modules already moved out of
%% BioPerl and into their own distributions.

\subsection{Dist::Zilla and Pod::Weaver}

%% Technically, the first one to be moved out of BioPerl was
%% Bio-Graphics and that was in 2009.  However, that was an
%% independent event, not part of a concerted effort of reorganising
%% BioPerl.  That only started in 2011 (see Changes file in
%% bioperl-live).
The large number of modules in BioPerl became a maintenance problem
so in 2011, a new project initiated to split BioPerl into
more manageable distributions such as Bio-Biblio, Bio-FeatureIO, and
Bio-Coordinate.  To reduce existing code, prevent duplication,
and to make new releases
a easier, Dist::Zilla and Pod::Weaver were adopted for
the new distributions.

Dist::Zilla is a program facilitating the writing, packaging, management,
and release of free software targeted at libraries written in the Perl
programming language and released to the Comprehensive Perl Archive
Network (CPAN) repository.  It comes with a series of plugins to automate the
release process such as the addition of copyright notices, discovery
of dependencies, and uploading to CPAN.

Pod::Weaver is a program to create documents in Plain Old
Documentation (POD) format, a format mainly used to write
documentation incorporated inlined within Perl modules.
Pod::Weaver includes a Dist::Zilla plugin
that serves as bridge between the two, so that most
standard generic POD content is
generated automatically as part of the release process.

As part of the restructure of the BioPerl distribution, we configured
the BioPerl Dist::Zilla plugin bundle, and created two new Pod::Weaver
section plugins, GenerateSection and Legal::Complicated.

GenerateSection creates POD sections based on templates.  It is
used in BioPerl to generate the support section of documentation
of individual modules
with distribution specific details such as links to the online
repository.

Legal::Complicated creates a POD section for the copyright details
based on comments in individual modules.  While BioPerl is free
software, individual modules may be released under different free
software licenses, and each has their own author who is often not
the copyright holder.

Both new Pod::Weaver plugins and the BioPerl PluginBundle are available
on CPAN since 2013.

\subsection{Bio-EUtilities}

For the analysis of the canonical histone gene
family \Crefp{ch:histone-catalogue} we required a tool to
automate the search of histone genes and download associated sequences.
We used the NCBI Gene database for searches and
created a new program withing the Bio-EUtilities package from BioPerl.

Gene is a public database hosted at the National Center for
Biotechnology Information (NCBI) which maps known or predicted genes
to other entries in the NCBI Reference Sequence (RefSeq).  Therefore, Gene
links to the Genome, Nucleotide, and Protein databases \citep{gene-database}.

Bio-EUtilities is part of the BioPerl project and provides a Perl
interface to NCBI's Entrez Programming Utilities (E-Utilities).
Entrez is a federated search engine for multiple databases of
biomedical data including Gene.  Entrez has an
interactive interface at \url{https://www.ncbi.nlm.nih.gov/} while
E-Utilities provides an equivalent
programming interface for queries using a fixed
URL syntax.

The program \command{bp\_genbank\_ref\_extractor} component was
created to take as input a query to the Entrez Gene database and
to downloads all genomic, transcript, and protein sequences as well
as a CSV file with chromosome coordinates, names, and identifiers as output.
It has several options such as download of flanking sequences, different
output formats, choice of genome assembly, and skipping of non coding
genes.  \command{bp\_genbank\_ref\_extractor}
is provided with an extensive manual covering all options
and examples \Arefp{app:pod-doc}.

\command{bp\_genbank\_ref\_extractor} was released with
Bio-EUtilities version 1.73.

\subsection{Debian packaging}

Debian is a computer operating system, the set of low-level software
that manages a computer hardware and resources for computer programs.
Debian is
composed entirely of free software and one of the earliest GNU/Linux
distributions.
Debian is package based like all free operating systems today.
This means that it is made of multiple components known as
packages.  For example, in Debian there are packages for the Linux
kernel, the Perl programming language, and Dist::Zilla.  Packages are
managed by a package management system which handles their
installation, configuration, and removal to simplify
a multiplicity of tasks which otherwise would
have to be handled individually by the user.

%% Debian Jessie has 21024 source packages.  The binary number of
%% packages would be much higher but while that number is what is
%% usually used when making a point for Debian's high number of
%% packages, it is misleading.  The point is how much of upstream
%% projects are packaged, and that is the number of source packages.

Debian is a widely used GNU/Linux
distribution with more than 21000 packages and a large
number of derivative distributions.  These new distributions
inherit the base of their packages from Debian, and some like Ubuntu
and Knoppix are in turn the parents of their own derivatives.  By packaging
for Debian, we effectively prepare packages for the whole family of
Debian based distributions.

While Debian had a package for the main BioPerl distribution, it did
not have one for Bio-EUtilities.  We packaged Bio-EUtilities for
Debian with the aim of making it easier for
others to reproduce our results.
Similarly, to make it easier for prospective BioPerl
developers, we packaged all the Dist::Zilla plugins required to produce
new BioPerl releases as well
as all the module distributions required by them
\trefp{tab:software:debian-packages}.

\begin{table}
  \captionIntro{Perl module distributions packaged for Debian}{}
  \label{tab:software:debian-packages}
  %% This one line descriptions were retrieved from Debian description
  %% which we wrote (except the one for Bio-EUtilities).
  \begin{description}
  \item[Bio-EUtilities] \hfill \\
    Webagent which interacts with and retrieves data from NCBI's E-Utils.
  \item[Config-MVP-Slicer] \hfill \\
    Module to extract embedded plugin config from parent config.
  \item[Dist-Zilla-Config-Slicer] \hfill \\
    Config::MVP::Slicer customized for Dist::Zilla.
  \item[Dist-Zilla-Plugin-AutoMetaResources] \hfill \\
    Dist::Zilla plugin to ease filling \command{resources} metadata.
  \item[Dist-Zilla-Plugin-MojibakeTests] \hfill \\
    Dist::Zilla plugin that provides author tests for source encoding.
  \item[Dist-Zilla-Plugin-ReadmeFromPod] \hfill \\
    Dist::Zilla plugin to generate a README from POD.
  \item[Dist-Zilla-Plugin-Test-Compile] \hfill \\
    Common tests to check syntax of Perl modules, using only core modules.
  \item[Dist-Zilla-Role-PluginBundle-PluginRemover] \hfill \\
    Dist::Zilla plugin to add \command{-remove} functionality to a bundle.
  \item[MooseX-Types-Email] \hfill \\
    Email address validation type constraints for Moose.
  \item[Pod-Weaver-Plugin-EnsureUniqueSections] \hfill \\
    Pod::Weaver plugin to check for duplicate POD section headers.
  \item[Pod-Weaver-Section-Contributors] \hfill \\
    Pod::Weaver plugin for a section listing contributors.
  \item[Pod-Weaver-Section-GenerateSection] \hfill \\
    Pod::Weaver plugin to add POD sections from a template text.
  \item[Pod-Weaver-Section-Legal-Complicated] \hfill \\
    Pod::Weaver plugin for per module authors, copyright holders, and license.
  \item[Test-Mojibake] \hfill \\
    Module to check source for encoding misbehaviour.
  \end{description}
\end{table}


\section{Build systems for reproducible research}

Even if the original data is available for a computational biology
investigation such as microscopy image or sequence analysis
and the runtime environment under
which the computational analysis was done can be duplicated, reproducing results
is dependent on invoking the same commands and same
options in the same order as the original analysis.
The necessary information to achieve this is often undocumented
and difficult to reconstruct.

A build system is a software tool that automates the process of
performing a complex series of steps for the generation of an artifact.
It is mainly used in software engineering to automate software
compilation and packaging
but the process of maintaining software has many parallels with
maintaining a reproducible research projects in computational biology
so the same tools
can be used.  In a software compilation project, object code is built from the
source code whereas in a research project, figures, tables, and values
are built from raw
data.  In a software project an executable program is built from multiple
object code whereas in a research project a manuscript is built from the
figures, tables, and values.

%% On ReDocs there is a set of rules to prepare figures and run the
%% analysis.  What he proposed was a set of names to generate such
%% figures, run analysis, skip analysis that would take too long, and
%% remove intermediary files.  This is similar to what GNUs coding
%% standarda mandates via automake, all Makefiles should support
%% install, all, help, check, so users know immediately what each one
%% does.
Claerbout and colleagues \citep{ReDoc-claerbout} proposed this
parallel and
coined the term ``reproducible research''.  They created a standard
build system for the generation of figures and manuscript from author
data and analysis software as extension to GNU
Make, one the most common build system.  Following from this,
Madagascar \citep{madagascar-scons} was developed as a
software solution specialised
for reproducible computational experiments, based on the software
build system SCons.

The SCons (from Software Construction) build
system \citep{scons} was designed to be a
replacement of Make and resembles Make in concept.  However, it has
the advantage of being configured using Python which is a modern
programming language often praised for its readability.

%% Other people may mention other SCons advantages but I disagree:
%% 1.SCons has builtin configure and dependency analysis.  Well, that
%%   is bullshit.  First, it doesn't work properly.  Configure support
%%   was a second thought and is very much incomplete.  Second,
%%   scanner is really slow.  And while Make itself really does not
%%   have them, it is meant to be used as part of Autotools where
%%   those jobs are done by autoconf and automake.
%% 2. SCons uses a liberal licence.  Well, I prefer copyleft and
%%    anyway that's the build system.
%%
%% One advantage that I could agree is default to md5sums instead of
%% timestamps but then I would have to explain what md5 and a checksum
%% is.  It also has builtin support for latex but that's really easy
%% to do in GNU Make.

For the work in \Cref{ch:histone-catalogue} we made extensive use of
the Perl programming language which is not supported by default in
SCons but commonly used in bioinformatics.
To simplify the use of SCons for our project we created
a SCons perl tool which adds automatic prerequesite
scanning, perl configuration options, and multiple functions for using
Perl scripts.

The SCons perl tool is available at
\url{https://bitbucket.org/carandraug/scons-perl5} under a free
software licence.

%% automatic prerequesite scanning -> Scanner
%% configuration options -> Variables
%% multiple functions -> all the perl builders.

\section{Conclusion}

Reproducible research has two reasons to be.  One is to allow other
researchers to perform the same analysis.  This can be on the same
dataset as the original research and identify possible errors on the
methodology.  The same analysis can also be performed on another
researcher dataset.  When associated with the freedom to inspect the
source code, it also enables one to check for errors directly on the
code.  There is nothing in this chapter to reproduce.

The second reason, and the one of most concern in this chapter, is to
enable others to use the new tools in new research directions.  This
requires a different approach to the development of scientific
software requiring more insight from software design and engineering.
It is not enough to developed a software that solves our problem
within the constraints of our problem, it must be configurable and
generalised so that it can fit in other users.  For example, the
Pod::Weaver plugins we created to solve the problem of making BioPerl
releases were implemented with extra options that are not needed.
They could have even been implemented not as separate plugins but
straight into the BioPerl plugin as if configuration.  However, having
them as separate components enabled other software projects to use
them.  Being free software we do not know exactly how many people use
it and can only assume, and we only hear when they don't work.  We
know people are using it because they have reported issues and even
improvements to us.  For the case of our Pod::Weaver plugins, this has
happened when users of our software were ahead of us in newer versions
and reported issued before we even got there.

Users reporting issues are signal of users interest on the software,
the more users we have the more issues are reported to us.  In that
light, our work in Octave is likely to have had the most impact,
issues are reported or discussed about the Octave Forge image package
several times a month.  However, in the case of Octave, our work has
become so entangled with the work with others contributors that we
wouldn't be able to measure the exact impact of contributors alone.
This, even if we had the information of how many people use Octave,
and how many use the Image package, and how many use the specific
functions we improved.

We recognize that full fledged programs impose limitations.  A program
is easier to use but the availability of it as a library allows for
reuse by other developers.
One other aspect of writing code with potential to be useful for
others is to write it modularity in mind so that it can be used in
other people programs.  For example, when writing \command{frapinator}
to perform our analysis of FRAP data, we wrote in such a way that the
program is just a shell around our Octave FRAP
package.  \command{frapinator} simply reads the options and then calls
a series of a functions from the frap package.  This will allow other
researchers to reuse the parts they need.  For example, they can pick
the functions to identify the bleach spot but use alternative frap
models, possibly from another such library.  Or vice-versa.

Going to the root of the problem is though and adds a lot of effort.
For example, our choice of work with Debian to package software we
needed for our projects stemmed of the difficult members of out group
had in installing some of the required dependencies.  We could have
just installed it on their behalf but that wouldn't benefeciary other
researchers.  It would also just happen again when they got a new
machine.  Another alternative would be to include the software on our
software.  This has its own problems.  halfway during our project,
NCBI moved the E-Utilities services to a encrypted protocol.  This was
fixed in Bio-Eutilities but if we had just included the code, then
this owuld not have been fixed.

These are the tools we wished existed at every turn of our research.
Had they already exist, we have no doubt we would be reporting other
tools that we would find missing at the turns we didn't reach.
