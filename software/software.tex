\chapter{Software Tools for Image and Sequence Analysis}

\todo[inline]{Add one page general background to broad needs and
  motivations of software.  Two sections on: Why software development
  is essential for cell biology (1 page), and Why FOSS is important for
  researchers (half page)}

\section{GNU Octave}

The GNU Octave programming language was heavily used for the analysis
of microscope images throughout this work.  Several features of the
language such as its handling of multi-dimensional arrays, interactive
top level, and extensive image processing functions, as well as
its supportive community make it an attractive choice
for quantitative microscopy.

Biological microscope images have a varying number of dimensions.
In addition to the 2 dimensional plane of a standard image, microscope
outputs typically include any combination of: $z$ dimension along
the optical axis for a volume image; time dimension for time-lapse
experiments; and wavelength for multi-channel experiments.  Recent microscopy
techniques generate images with an even higher number of dimensions
such as angle, phase, and lifetime.  This varying dimensionality adds
complexity and makes it challenging to write generalized subroutines for
microscope image analysis.

%% All values are multi-dimensional arrays does not include objects
%% wrapped from other languages such as java objects (and I guess it
%% will be the same for python when the pytave project is finished).
GNU Octave is an array programming language, high level programming
languages where operations are generalised to multidimensional arrays,
primarily intended for numerical computations \citep{octave}.
In Octave, all values are multidimensional arrays which provides
an abstraction layer that is useful when writing code for an arbitrary
number of dimensions as is the case for microscope image processing.

%% Doesn't seem to be any scientific study about REPL as an advantage
%% for prototyping.  Similarly, none about being better for
%% exploratory data analysis but I can't imagine anyone disagreeing
%% with it.
Octave also has a Read--Eval--Print Loop (REPL), or interactive top
level, like IPython and Unix shells, which reduces the
feedback time and provides an efficient
environment for exploratory data analysis.

%% No citation for this, only the anecdotal evidence of being the
%% project leader of octave Forge for 6 years.
The project has a large and active community
mainly composed of
scientists and engineers as a large support group of
specialists in numerical computations.
The separate Octave Forge project hosts a vast number of
packages that extend Octave into specific applications such as control
systems \citep{octave-control}, time-frequency analysis
\citep{octave-ltfat}, or level sets \citep{octave-level-set}.
It provides a collaborative environment for development of Octave
packages and another nexus of the Octave community.

Both GNU Octave and Octave Forge packages are free software which allows
the study, modification, and distribution of modified source.
We made extensive
use of this feature and have contributed to improving Octave and its
packages for the needs of quantitative microscopy.

Finally, Octave comes with basic support for image processing and the
Octave Forge image package extends it with a number of functions for
image processing such as geometric transformations, mathematical
morphology, image registration, and noise reduction.

%% These were actually lsm files but those are actually TIFFs.
While Octave is well suited to image
processing, we also identified several problems always related with
the large image size or number of dimensions.  Our microscopy images
were several megabytes in size.  For example, the single cell
FRAP experiments for histone dynamics generating TIFF files of 449
These images had a field of view of 300
by 300 pixels and 2500 time frames
on acquisition by an 8-bit camera.
Such files sizes are typical in the field of microscopy.

\subsection{Reading and Writing of Image Files}

%% Number of 88 major image formats taken from GraphicsMagick
%% documentation on February 2017.
Octave uses the GraphicsMagick C++ library for the reading and writing
of image files which provides a common interface
to a varied collection of
image format specific libraries covering almost
90 major image formats.  In Octave, this functionality is
provided via the functions \command{imfinfo}, \command{imread}, and
\command{imwrite}.

%% The importance of reading all planes in a single call to imread is
%% of performance for TIFF.  This is because of the structure of TIFFs
%% where reading plane N means parsing all the IFDs before that.
These three functions were rewritten with the aim of achieving
reduced memory usage, increased performance, improved
interface for multidimensional images, and new image types.
New options were added to read and write a series of image planes
in a single function call, to read specific regions of interest in
individual planes, to read and write images
with floating point precision, and to append additional planes to existing image
files.

As part of this rewrite, new features were introduced such as
support for the CMYK colour model, EXIF
and GPS metadata, reading and writing of animations in GIF images, and
control over the image compression type,
while existing features were improved such as
support for transparency and
indexed images.

%% We can't use the term image IO functions because then we would have
%% to explain what IO is.  For the same reason, we can't say ``hooks''.
To support this development, we created a system that allows changing
how the \command{imread}, \command{imwrite}, and \command{imfinfo}
functions behave for each file format.
This enables Octave extensions to add support for new image file
formats or to improve access to the existing formats
without addition of new format specific functions.
For example, it is now possible for packages to enable reading of
microscope specific metadata from \command{imfinfo}.
This system is available via the function \command{imformats}.

Changes extended to other functions of Octave related to images
including  functions involved in conversion between color
models, grayscale images, and indexed images to support integer and
floating point data types, and multiple dimensions
\trefp{tab:software:octave-core-functions}.

All changes were released with Octave version 3.8.0.

\begin{table}
  %% This table includes the major things we contributed to Octave
  %% core.  It doesn't mention a version since it's work I contributed
  %% between Octave 3.6 and 4.2.  Some of the changes even span
  %% multiple versions.
  %% It does not include minor bug fixes or trivial bug fixes.  Also
  %% does not include major changes we did to the internals which only
  %% reduced code duplication.
  \captionIntro{New or improved functions in GNU Octave}{}
  \label{tab:software:octave-core-functions}
  \begin{multicols}{3}
    \begin{itemize}[label={}]
      \foreach \function in {
        bitcmp,
        bzip2,
        cubehelix,
        fliplr,
        flipud,
        flip,
        frame2im,
        gallery,
        gray2ind,
        gzip,
        hsv2rgb,
        im2double,
        im2frame,
        imfinfo,
        imformats,
        imread,
        imwrite,
        ind2gray,
        ind2rgb,
        inputParser,
        ntsc2rgb,
        pkg,
        psi,
        rectint,
        rgb2hsv,
        rgb2ind,
        rgb2ntsc,
        rot90,
        validateattributes}
      { \item \command{\function} }
    \end{itemize}
  \end{multicols}
\end{table}

\subsection{Bio-Formats and Octave Java interface}

%% We kind of managed to read the pixel data.  This also required
%% knowing that LSM interleaves the actual pixel data with thumbnails
%% which need to be skipped.
While GraphicsMagick provides support to read many image formats, its
support for scientific microscope image formats is limited.
For example, Zeiss confocal microscopes saves
images in the LSM file format which is a proprietary extension of TIFF.
Although GraphicsMagick reads LSM pixel data,
the file metadata such as pixel size, time interval, or region of
bleaching event cannot be retrieved.

Bio-Formats is a free software library for reading and writing image
data with a strong focus on microscopy image file formats
\citep{bioformats}.  It is written in the Java programming language and
used by other programs in the field of microscope image analysis such
as CellProfiler \citep{cellprofiler}, ImageJ \citep{imagej2}, and OMERO
\citep{omero}.

%% Before version 3.8.0, there was a separate Octave Forge package
%% that added the java interface.  Actually, Octave version 3.8.0
%% pretty much only merged the java package into it.
Octave has a native interface to the Java programming language
that should enable easy integration with
Bio-Formats, and Bio-Formats has a Matlab toolbox that should be
compatible with Octave.
We identified a series of problems in the Octave interface to Java that
could be solved either by improving Octave or Bio-Formats.

%% Writing this paragraph without mentioning the words data type,
%% primitive, wrapper, and objects is though.  And on top of that,
%% writing it without sounding dumb is just impossible.
In Octave, we rewrote the handling of values returned from Java.
Of special importance for BioFormats was the automatic conversion of
Java arrays which are used to return pixel data.
Support for conversion of multidimensional Java arrays was not implemented
since we did not require it and it remains available for a future project.

In Bio-Formats, we modified their Matlab toolbox so that it is
both Octave and Matlab compatible.
This change simplifies the packaging of
Bio-Formats for Matlab and Octave making them
effectively the same code.
Finally, we automated the creation of Octave packages in Bio-Formats
so that they be made part as part of Bio-Formats releases.

All changes were released with Octave since version 4.0.0 and
Bio-Formats since version 5.1.2

\section{Octave Forge image package}

The Octave programming language is primarily intended for numerical
computations and provides a syntax and set of functions particularly
convenient for solving linear algebra and differential equations.  It
also includes a collection of functions for the handling of image data
but those are focused on the reading and writing of image files,
conversion between colour models, and graphical display.

The Octave Forge image package supplements Octave with a wide range of
image analysis specialised functions.  While Octave syntax is
identical for any number of dimensions and data type, we found that
many of the image package functions were either limited to two
dimensional images or inefficient for the large images we obtained in
microscopy.  We began a project with the purpose of eliminating any
limitation on number of dimensions in the image package
\trefp{tab:software:octave-image-functions} which has seen multiple
releases, starting in version 2.0.0 and continuing in the current
version 2.6.1.
%% Other issues are when functions just treated higher number of
%% dimensions as a long 2D image and introduced artifacts on image
%% borders.  Or they looped over each pixel making it terribly slow
%% even on 2d images.
%%
%% The first time the project to support ND images was made official
%% was for the summer of 2013 (my GSOC) but even on that proposal I
%% mentioned work already made.

\begin{table}
  \captionIntro{New or improved functions in the Octave Forge image
    package}{}
  \label{tab:software:octave-image-functions}
  \begin{multicols}{3}
    \begin{itemize}[label={}]
      \foreach \function in {
        bestblk,
        bwareafilt,
        bwareaopen,
        bwconncomp,
        bwdist,
        bwlabel,
        bwlabeln,
        bwmorph,
        bwperim,
        bwpropfilt,
        checkerboard,
        col2im,
        colfilt,
        conndef,
        edgetaper,
        fftconv2,
        fftconvn,
        grayslice,
        graythresh,
        im2col,
        imabsdiff,
        imadjust,
        imattributes,
        imbothat,
        imclearborder,
        imclose,
        imcomplement,
        imcrop,
        imdilate,
        imerode,
        imfill,
        imgetfile,
        imhist,
        imlincomb,
        immse,
        imopen,
        impixel,
        impyramid,
        imquantize,
        imreconstruct,
        imregionalmax,
        imregionalmin,
        imresize,
        imrotate,
        imtophat,
        intlut,
        iptcheckconn,
        label2rgb,
        labelmatrix,
        mat2gray,
        mmgradm,
        montage,
        nlfilter,
        normxcorr2,
        ordfiltn,
        otf2psf,
        padarray,
        psf2otf,
        psnr,
        regionprops,
        rgb2ycbcr,
        strel,
        stretchlim,
        subimage,
        tiff\_tag\_read,
        watershed,
        wavelength2rgb,
        ycbcr2rgb}
      { \item \command{\function} }
  \end{itemize}
  \end{multicols}
\end{table}

\subsection{Image Thresholding Algorithms}

Image thresholding is a method for image segmentation where the image
pixels are separated into classes based on their intensity values.
The simplest of the thresholding methods uses a single fixed value and
separates an image into background and foreground which are then
represented in a binary, or black and white, image.  This is
particularly useful in images of fluorescence microscopy where objects
of interested show bright with high intensity values against a dark
background with low intensity values.

Multiple threshold algorithms exist to automate the choice of a
threshold value, the most common of which being based on the analysis
of an image histogram \citep{analysis-hist-threshold}.  Otsu's method
\citep{otsu-threshold} is among the most widespread of this methods
and is available in the function \command{graythresh} of the image
package.  We rewrote this function to handle histograms of arbitrary
length, vectorised for performance, and additional computation of a
``goodness'' of the threshold value.  In addition, we ported to Octave
the HistThresh toolbox, an unpublished collection of histogram
threshold algorithms by Antti Niemistö, making them available as an
option for \command{graythresh}.  Finally, the rewrite added the
option of using an histogram as input instead of an image, enabling
the possibility of histogram processing as a preceding step.

%% We have no reference for Antti Niemistö work.  It was never
%% published and only available on his University webpage at
%% http://www.cs.tut.fi/~ant/histthresh/ which is no longer available.
%% He has several other publications but none about this topic.
%% Here's one: A. Niemistö, V. Dunmire, O. Yli-Harja, W. Zhang, and
%% I. Shmulevich, "Robust quantification of in vitro angiogenesis
%% through image analysis," IEEE Transactions on Medical Imaging,
%% vol. 24, no. 4, pp. 549-553, April 2005.
%%
%% The only copies of his work that I know of are on my github account
%% (which I have since modified), and on https://archive.org/web

All this changes have been released with the Octave Forge image
package version 2.0.0.

\subsection{Mathematical Morphology}

%% This is not the complete story.  When we started, imdilate and
%% imerode had some support for ND images.  I can't quite remember
%% what was missing but I do remember making a table at the end with
%% performance comparison, and some comparisons couldn't be made
%% because they were missing before.  Looking at the source for 1.0.15
%% (before I started this), I can see that imerode and imdilate called
%% ordfiltn for grayscale images which was bound to be quite slow, and
%% filter2 for binary images which would limit to 2d images.
Mathematical morphology is the theory for the analysis of spatial
structures which provides a powerful image analysis technique based on
the shape and form of objects.  This is achieved by probing an image
with a known shape, named Structuring Element (SE), and filtering the
image based on whether the SE fits, or does not fit, on each location
within the image.

The fundamental morphological operations are named dilation and
erosion which are available in the image package through the
\command{imdilate} and \command{imerode} functions.  More complex
morphological operations are built on top of this two operations.  For
example, morphological top-hat transform corresponds is the difference
between an image and its morphological opening which in turn
corresponds to an erosion followed by a dilation.

Being the two fundamental operations, they are the ideal targets of
improvements for mathematical morphology since any performance
improvement and support for new image types will be propagated to
other morphology functions.

%% The original imdilate and imerode used __spatial_filtering__ with
%% the ordefiltn option.  We first optimised this by using convn which
%% made it a lot faster but only for binary images.  We then modified
%% __spatial_filtering__ so that it was faster in grayscale images
%% too.  We ended up writing a specialised function for dilation and
%% erosion.

%% My notes say that perfomance gains were between 1.5-30x for erosion
%% and dilation, and 1/5 to 2/5 faster for __spatial_filtering__.  At
%% least the erosion and dilation were on my old blog which got lost
%% after the server was damaged.
%% SE specialisation == SE decomposition
\command{imdilate} and \command{imerode} used the general purpose
\command{\_\_spatial\_filtering\_\_} function of the image package.
We rewrote the functions aimed at morphology operations with
specialisations for different data types and SE.  For the SE
specialisation, a new \command{strel} class was created which also
supported both flat and non-flat SE.  This changes were released with
the Octave Forge image package version 2.2.0.

%% Not mentioning the rewrite of __spatial_filtering__ because the
%% only reason to do so was performance.  While my notes mention nice
%% values, I don't have the time to make proper performance comparison
%% so better not even talk about it.

\subsection{Image Regions of Interest}

The function \command{regioprops} is used to measure different
properties in image regions of interest such as a region centroid,
area, or eccentricity.  However, this function was inappropriate for
the measurement of multiple regions and properties because the whole
image was analysed again for each region and property.  We rewrote
this function.  The computation of area and intensity weighted
centroid of 2000 regions which was not finished overnight in a
standard desktop computer was finish in under 3 minutes after our
rewrite.  This was released with the image package version 2.6.0.
%% The images I used for testing were Ezequiel's histone modification
%% markers, the images were 512x512x30 with slightly more than 2000
%% very small objects.

%% Actually there was a version of bwconncomp before but it didn't do
%% this, it only identified the boundaries of each object.  So while
%% technically, this function name existed, it was something else.
%% Our first attempt made use bwlabeln but then it had to undo the
%% labelling which is inefficient.  That was released with version
%% 2.2.0.  The writing of it as a function performing flood-fill was
%% done later and released with image 2.4.0.
We also wrote the function \command{bwconncomp} to perform the
identification of image regions with any number of dimensions.  It
creates an array of indices for the image regions which is an
alternative to labelled images with reduced memory usage.  It is used
internally by \command{regionprops}.  It was released with the image
package version 2.4.0.

%% Other Octave that I am not mentioning:
%%   * move all XMatrix as subclasses of XNDarray to reduce code
%%     duplication.
%%  * liboctave better integration with STL
%%  * templates for svd instead of duplicated classes
%%  * cleanup of methods (capacity, length, nzmax)
%%  * making Octave quiet when calling scripts of eval.
%%  * zero length dimensions on broadcasting
%%  * accumarray, zeros, ones, etc with empty lists
%%  * rewrite 'pkg build' for distribution of binary packages.
%%  * miscellaneous package: textable and units
%%  * signal package: xcorr2 coeff option
%%  * statistics: hist3, grp2idx, squareform, and binostat
%%  * mapping: a bunch of low hanging fruits
%%  * dicom: add support for any version of gdcm.
%%
%%  * all the work on the zenity package.  Maybe I will still write
%%    about it...
%%  * normxcorr2 in the image package.

\section{Octave FRAP package}
\label{sec:software:octave-frap}

For the work in \Cref{ch:kill-frap}, we required tools to estimate
binding constants from FRAP recovery data.  We obtained the code for a
previously reported circle FRAP model \citep{mcnally-frap-code} from
personal communication with the authors under the GNU General Public
Licence (GPL) version~3 or later.  This model includes multiple
parameters and considers the profile of the photobleach, correction
for observational photobleaching, finite size of the nucleus, and
fitting to both a pure-diffusion model and a full model with binding
states.

This code was written in the Matlab programming language and was
easily ported to Octave, the main difference being the replacement of
the nonlinear fitting from \command{nlinfit} with \command{leasqr}
from the Octave Forge optim package since both performs the same
algorithm, Levenberg--Marquardt nonlinear least squares.

Identification of the bleach spot, nucleus, and background regions are
required for the circle FRAP model.  The bleach spot measures
intensity recovery and also models the photobleach profile since it
takes into account a non-uniform spatial distribution of the bleached
spot.  The nucleus region defines the finite sized nucleus and takes
into account the fluorescence loss due to observational
photobleaching.  A small region outside the nucleus is used for
background correction.

The identification of these regions was performed manually which in
the interest of batch processing we automated.  The bleach spot was
identified in the difference between the post and pre-bleach frames.
Individual nuclei were segmented after automatic threshold with Otsu's
method.  A background region was identified as the rectangle of a
fixed size with lowest average intensity in the image.

With all the steps automated, we created a single program in Octave
which we named \command{frapinator} that would perform all the
analysis for any number of images.  The multiple options were made
available via command line options so that the user does not need to
modify the source code.  To quickly filter out any faulty analysis,
two images are created that work as a visual log, one showing the
automatically identified regions \frefp{fig:kill-frap:roi} and another
the recovery curves, intermediary analysis, and best fits
\frefp{fig:kill-frap:frapinator}.

\begin{figure}
  \centering
  \subbottom[pre-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-prebleach.png}
                    \label{subfig:software:frap-roi:prebleach}
  }
  \hfill
  \subbottom[post-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-postbleach.png}
                    \label{subfig:software:frap-roi:postbleach}
  }
  \subbottom[pre-bleach $-$ post-bleach]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-subtracted.png}
                    \label{subfig:software:frap-roi:subtracted}
  }
  \hfill
  \subbottom[Identified ROIs]{
    \includegraphics[width=0.45\textwidth]
                    {results/roi-selected.png}
                    \label{subfig:software:frap-roi:selected}
  }
  \captionIntro{Automatic selection of regions for FRAP analysis}
               {HeLa stable cell line expressing the H4~R45H mutant
                 tagged with YFP, are imaged every \SI{30}{\ms} in a
                 confocal microscope. A circular shape is used for
                 photobleaching after 100~frames.
                 \subcaptionref{subfig:software:frap-roi:prebleach}
                 averaging of 50~pre-bleach images removes most of the
                 noise, allowing for a better refined ROI;
                 \subcaptionref{subfig:software:frap-roi:postbleach}
                 average of 5 post-bleach images;
                 \subcaptionref{subfig:software:frap-roi:subtracted}
                 subtraction of the post-bleach to the pre-bleach
                 image, gives a clear indication of the bleach spot,
                 as well as faint signal for the nuclear region due to
                 unintentional photobleaching;
                 \subcaptionref{subfig:software:frap-roi:prebleach}
                 perimeter of the automatically identified ROIs
                 superimposed on the pre-bleach image: cell nuclei,
                 bleach spot, and background region.  }
               \label{fig:software:frap-roi}
\end{figure}

\begin{sidewaysfigure}
  \includegraphics[width=\textwidth]{results/frapinator.png}
  \captionIntro{Frapinator visual log files for batch processing}
               {Each FRAP experiment generates a log file with 6
                 different plots displaying the analysed values and
                 the fitting to different models.  In conjunction with
                 the images in \fref{fig:software:frap-roi} this
                 provides a quick overview of the entire analysis
                 process.  The top left plot displays the raw
                 intensity for the background, bleach spot, and
                 nucleus intensity over the duration of the FRAP
                 experiment. This is followed by the normalized
                 intensity for the bleach spot which is actually used
                 for the fitting. The top right displays the intensity
                 profile for the bleach spot, and its fit to a radial
                 profile model. The three bottom panels display the
                 data fitted to three different models: pure diffusion
                 which has no terms for binding constants; full model
                 with a fixed diffusion rate; and full model with all
                 the 3 terms.  }
               \label{fig:software:frapinator}
\end{sidewaysfigure}

We packaged the \command{frapinator} program and all the FRAP analysis
individual functions into an Octave FRAP package and made it available
online at \url{https://github.com/carandraug/octave-frap}.

\section{BioPerl}

The BioPerl project is an international association of developers of
free Perl software for bioinformatics, genomics, and life science
\citep{bioperl}.  It has created the BioPerl distribution of Perl
modules which has almost 800 modules for management and manipulation
of biological data to programmatic access of databases such as GenBank
and SwissProt, and bioinformatics tools such as ClustalW and Blast+.

%% $ find bioperl-live/Bio/ -name '*.pm' -type f | wc -l
%% 791
%%
%% This number no longer includes the modules already moved out of
%% BioPerl and into their own distributions.

\subsection{Dist::Zilla and Pod::Weaver}

%% Technically, the first one to be moved out of BioPerl was
%% Bio-Graphics and that was in 2009.  However, that was an
%% independent event, not part of a concerted effort of reorganising
%% BioPerl.  That only started in 2011 (see Changes file in
%% bioperl-live).
The large number of modules in BioPerl became a maintenance problem
so in 2011, a new project to split BioPerl into
more manageable distributions such as Bio-Biblio, Bio-FeatureIO, and
Bio-Coordinate.  To reduce existing code, prevent duplication,
and to make new releases
a easier, Dist::Zilla and Pod::Weaver were adopted for
the new distributions.

Dist::Zilla is a program to make it easier to write, package, manage,
and release free software targeted at libraries written in the Perl
programming language and released to the Comprehensive Perl Archive
Network (CPAN) repository.  It comes with a series of plugins to automate the
release process such as the addition of copyright notices, discovery
of dependencies, and uploading to CPAN.

Pod::Weaver is a program to create documents in Plain Old
Documentation (POD) format, a format mainly used to write
documentation inlined in Perl modules.
Pod::Weaver includes a Dist::Zilla plugin
that serves as bridge between the two, so that most boilerplate POD is
generated automatically as part of the release process.

As part of the restructure of the BioPerl distribution, we configured
the BioPerl Dist::Zilla plugin bundle, and created two new Pod::Weaver
section plugins, GenerateSection and Legal::Complicated.

GenerateSection creates POD sections based on templates.  It is
used in BioPerl to generate the support section of documentation
of individual modules
with distribution specific details such as links to the online
repository.

Legal::Complicated creates a POD section for the copyright details
based on comments in individual modules.  While BioPerl is free
software, individual modules may be released under different free
software licenses, and each has their own author who is often not
the copyright holder.

Both new Pod::Weaver plugins and the BioPerl PluginBundle are available
on CPAN.

\subsection{Debian packaging}

Debian is a computer operating system, the set of low-level software
that manages a computer hardware and resources for computer programs.
Debian is
composed entirely of free software and one of the earliest GNU/Linux
distributions.
Debian is package based like all free operating systems today.
This means that it is made of multiple components known as
packages.  For example, in Debian there are packages for the Linux
kernel, the Perl programming language, and Dist::Zilla.  Packages are
managed by a package management system which handles their
installation, configuration, and removal to simplify tasks which otherwise would
have to be handled by the user.

While Debian had a package for the main BioPerl distribution, it did
not have one for Bio-EUtilities.  We packaged Bio-EUtilities for
Debian with the aim of making it easier for
others to reproduce our results.
Similarly, to make it easier for prospective BioPerl
developers, we packaged all the Dist::Zilla plugins required to produce
new BioPerl releases as well
as all the module distributions required by them
\trefp{tab:software:debian-packages}.

\begin{table}
  \captionIntro{Perl module distributions packaged for Debian}{}
  \label{tab:software:debian-packages}
  %% This one line descriptions were retrieved from Debian description
  %% which we wrote (except the one for Bio-EUtilities).
  \begin{description}
  \item[Bio-EUtilities] \hfill \\
    Webagent which interacts with and retrieves data from NCBI's E-Utils.
  \item[Config-MVP-Slicer] \hfill \\
    Module to extract embedded plugin config from parent config.
  \item[Dist-Zilla-Config-Slicer] \hfill \\
    Config::MVP::Slicer customized for Dist::Zilla.
  \item[Dist-Zilla-Plugin-AutoMetaResources] \hfill \\
    Dist::Zilla plugin to ease filling \command{resources} metadata.
  \item[Dist-Zilla-Plugin-MojibakeTests] \hfill \\
    Dist::Zilla plugin that provides author tests for source encoding.
  \item[Dist-Zilla-Plugin-ReadmeFromPod] \hfill \\
    Dist::Zilla plugin to generate a README from POD.
  \item[Dist-Zilla-Plugin-Test-Compile] \hfill \\
    Common tests to check syntax of Perl modules, using only core modules.
  \item[Dist-Zilla-Role-PluginBundle-PluginRemover] \hfill \\
    Dist::Zilla plugin to add \command{-remove} functionality to a bundle.
  \item[MooseX-Types-Email] \hfill \\
    Email address validation type constraints for Moose.
  \item[Pod-Weaver-Plugin-EnsureUniqueSections] \hfill \\
    Pod::Weaver plugin to check for duplicate POD section headers.
  \item[Pod-Weaver-Section-Contributors] \hfill \\
    Pod::Weaver plugin for a section listing contributors.
  \item[Pod-Weaver-Section-GenerateSection] \hfill \\
    Pod::Weaver plugin to add POD sections from a template text.
  \item[Pod-Weaver-Section-Legal-Complicated] \hfill \\
    Pod::Weaver plugin for per module authors, copyright holders, and license.
  \item[Test-Mojibake] \hfill \\
    Module to check source for encoding misbehaviour.
  \end{description}
\end{table}

%% Debian Jessie has 21024 source packages.  The binary number of
%% packages would be much higher but while that number is what is
%% usually used when making a point for Debian's high number of
%% packages, it is misleading.  The point is how much of upstream
%% projects are packaged, and that is the number of source packages.

The choice of Debian was strategic.  Debian is a widely used GNU/Linux
distribution with more than 21000 packages and a large
number of derivative distributions.  These new distributions
inherit the base of their packages from Debian, and some like Ubuntu
and Knoppix, are in turn the parents of their own derivatives.  By packaging
for Debian, we effectively prepare packages for the whole family of
Debian based distributions.

\subsection{Bio-EUtilities}

For the work in \Cref{ch:histone-catalogue}, we required a tool to
automate the search of histone genes and download associated sequences.
We used the NCBI Gene database for the searches and
created a new program for Bio-EUtilities from BioPerl.

Gene is a public database hosted at the National Center for
Biotechnology Information (NCBI) which maps known or predicted genes
to other entries in the NCBI Reference Sequence (RefSeq).  Gene therefore
links to the Genome, Nucleotide, and Protein databases \citep{gene-database}.

Bio-EUtilities is part of the BioPerl project and provides a Perl
interface to NCBI's Entrez Programming Utilities (E-Utilities).
Entrez is a federated search engine for multiple databases of
biomedical data including Gene.  Entrez has an
interactive interface at \url{https://www.ncbi.nlm.nih.gov/} while
E-Utilities provides an equivalent
programming interface for queries using a fixed
URL syntax.

The program \command{bp\_genbank\_ref\_extractor} component we
created requires a query to the Entrez Gene database and
then downloads all genomic, transcript, and protein sequences as well
as a CSV file with chromosome coordinates, names, and identifiers.  It
has several options such as download of flanking sequences, different
output format, choice of genome assembly, and skipping of non coding
genes.  It is provided with an extensive manual covering all options
and examples \Arefp{app:pod-doc}.
\command{bp\_genbank\_ref\_extractor} was released with
Bio-EUtilities version 1.73.

\section{Build systems for reproducible research}

Even if the original data is available and the runtime environment under
which the computational analysis was done can be duplicated, reproducing results
is dependent on invoking the same commands and same
options in the same order as the original analysis.
The necessary information to achieve this is often undocumented
and difficult to reconstruct.

A build system is a software tool with the purpose of automating such
steps.  It is mainly used in software engineering to automate software
builds but the process of maintaining software has many parallels with
from maintaining a reproducible research projects so the same tools
can be used.  In a software project, object code is built from the
source code whereas in a research project, figures, tables, and values
are built from raw
data.  In a software project an executable program is built from multiple
object code whereas in a research project a manuscript is built from the
figures, tables, and values.

%% On ReDocs there is a set of rules to prepare figures and run the
%% analysis.  What he proposed was a set of names to generate such
%% figures, run analysis, skip analysis that would take too long, and
%% remove intermediary files.  This is similar to what GNUs coding
%% standarda mandates via automake, all Makefiles should support
%% install, all, help, check, so users know immediately what each one
%% does.
Claerbout \citep{ReDoc-claerbout} was the first who recognised this,
coined the term ``reproducible research'', and proposed a standard
build system for the generation of figures and manuscript from author
data and analysis software.  This new system was an extension to GNU
Make, one the most common build system.

Madagascar \citep{madagascar-scons} was another a software specialised
for reproducible computational experiments, based on the software
build system SCons.
The SCons (from Software Construction) build
system \citep{scons} was designed to be a
replacement of Make and it still resembles Make in concept.  However, it has
the advantage of being configured by Python which is a modern
programming language often praised for its readability.

%% Other people may mention other SCons advantages but I disagree:
%% 1.SCons has builtin configure and dependency analysis.  Well, that
%%   is bullshit.  First, it doesn't work properly.  Configure support
%%   was a second thought and is very much incomplete.  Second,
%%   scanner is really slow.  And while Make itself really does not
%%   have them, it is meant to be used as part of Autotools where
%%   those jobs are done by autoconf and automake.
%% 2. SCons uses a liberal licence.  Well, I prefer copyleft and
%%    anyway that's the build system.
%%
%% One advantage that I could agree is default to md5sums instead of
%% timestamps but then I would have to explain what md5 and a checksum
%% is.  It also has builtin support for latex but that's really easy
%% to do in GNU Make.

For the work in \Cref{ch:histone-catalogue} we made extensive use of
the Perl programming language which is not supported by default in
SCons but commonly used in bioinformatics.
To simplify the use of SCons for our project we created
a SCons perl tool and made available at
\url{https://bitbucket.org/carandraug/scons-perl5} under a free
software licence.  The SCons perl5 tool adds automatic prerequesite
scanning, perl configuration options, and multiple functions for using
Perl scripts.

%% automatic prerequesite scanning -> Scanner
%% configuration options -> Variables
%% multiple functions -> all the perl builders.
