%% \epigraph{tl;dr}{}
%% \epigraph{when the going gets tough, the tough get cardboard sleeves because the
%%           cups too hot.}{Doghouse diaries #5051}

Research on research is a very recent topic, after a series of recent
studies showing low reproduciblity rates on published research.  The
biggest studies on this have been on the field of psychology and
computational science, but the same trend appears to be in the medical
sciences.  The main cause for reproducibility has been the absence of
data and lack of details on methodology, namely on the analysis steps.
In the field of computational science, the main issue is the absence
of the code developed.

In this thesis, we attempt to follow the best practices for
reproducibility.  Figures and tables can be traced back to the raw
data and the code used to generate them has been made available.  In
\Cref{ch:histone-catalogue}, references to values are inserted in the
text as part of the analysis to avoid getting out of sync as the data
changes and we improve our analysis.  In \Cref{ch:kill-frap}, the
individual images, regions, and scaling options used to create the
inset figures are all made public.  Even in the introduction, we
automated the generation of figures.  For example,
\fref{fig:intro:histone-fold-domain} can be regenerated from the
crystal structure, as we include PyMOL scripts for their creation, and
\fref{fig:intro:frap-curve-example} has the TikZ source code so that
others can improve it.  There is one exception.  Figures in
\Cref{ch:kill-frap} from the deltavision were deconvolved after
acquisition by SoftWoRx which is non-free software.  As this was the
very first step after image acquisition, we trace all the steps to the
deconvolved images.

%% \todo{There's one other exception, the FACS figure
%%   in the methods chapter.  At the time I couldn't make the figure
%%   myself, but now I found an R package to read the facs file.  There
%%   is also a python package but it's abandoned.  Or we could just
%%   remove the figure, it's not that important anyway.}

The entire development of this thesis is available online as a git
repository at \url{https://github.com/carandraug/phd-thesis}.  The use
of a version control system provides access not only the thesis at the
point of submission, but also its whole history.  This is of less use
in a PhD thesis which is done at the end of the project.  However,
\Cref{ch:histone-catalogue} has been written as an individual
manuscript with the same ideals and is online at
\url{https://github.com/af-lab/histone-catalogue}.  The core
representation of an histone gene in that project has gone through
three major revisions and individual analysis scripts were constantly
modified.  A version control system associates each change with a
message so that each line of the analysis can be traced back in
history to an explanation on why it was done that way.

Finally, we used a software build system to automate the analysis
steps with the right options.  This still requires someone reproducing
our results to install all the used software, a task that is not
trivial.  To identity this issue as early as possible, we include
configure scripts which are run at the start of the build system and
identify any missing tool or feature.  GNU/Linux distributions ease
the task of installing the multiple tools we require by having a
package manager and we have contributed to this by packaging for
Debian some of the ones we needed.

\section{Data in Flux}

We found that our histone catalogue project was specially well suited
for this approach.  Not only are all the steps of a computational
nature without any wet-lab work, the transitive nature of the data
makes the automated nature of the reproducible approach interesting
beyond the sake of just saying it is reproducible.

When we started the histone catalogue project, the last publication on
the subject was from 2002 and was becoming outdated.  We initialised
this project with the aim of making a catalogue that would be
constantly up to date.  One year later, the Histone database was published, a
website with an up to date database specialized for histones.
However, no update has been done to it until 2016 with the version 2.0 of
the database.  We can only guess that no update will be
done in the next few years, and dependent on the group having receiving
funding to do it.

The histone database is a manually curated database of the histones
genes.  While this means it is the results from a fixed point in time,
the manual curation would suggest an higher quality.  Our automated
approach is also inferior in that it is dependent on the
annotation of the existing genes while the histone database database looked for
sequences with specific predicted structural motifs.  However, this is
a matter of encoding their logic as part of the build system.

The main point of our approach is that readers will have access to the
code.  They will not be limited to download the sequences we found,
they can get the sequences as they are now, and they can modify the
code to perform alternative analysis.  For example, our dependency on
current annotation could be replaced by predicted structural motifs as
was done by the histone database or by some new more elegant approach.
The design of our catalogue was
modular such that the source of data is just a block where the rest of the
analysis sits.  This block could be replaced while using the rest of
the infrastructure.  Similarly, all tables and figures are individual
blocks, new tables and figures could be added to a new catalogue.
By making the whole code public we are providing
anyone with all we can possibly offer to expand it.
We would take it as a compliment, and not feel sour and scooped, if another
research group published the next version of the histone catalogue
with this proposed changes.

We experienced this ourselves.  The initial purpose of the catalogue
was a table of all human histone genes and the differences between
their coding sequences.  With time, we kept adding other figures.
We got curious about the histone unique regulatory elements which
led to the addition of an option to download downstream sequences in
Bio-EUtilities and then to the identification of multiple regulatory
elements that were not annotated in the reference sequences.  We were
happy to find NCBI quite responsive to such corrections who quickly
made live our submissions.  We then created a new table that compared
the current annotations with the assumptions of histone genes and
made a list of anomalies.

We also tasted the advantage of modifying the code ourselves for new
purposes.  The original catalogue was also limited to human genes.
When we later tried it on the mice genome where it failed.  We changed
the catalogue to make it organism independent.  We envision that a
similarly, the catalogue could be adapted to another gene family.

While researches can, and we hope they will, work on top of our
histone catalogue, the level of skill required to reproduce it is not
common on a cell biologist.  Without a local bioinformatician, the
usefulfness of this power is lost.  For situations where data is in
flux as is the case of unversioned public databases, the software
principle of continuous delivery could be applied.  In this concept,
software is being developed and is always in a state that can be
released.  The releases are automated and anyone can download the
release from the last change.  Compared to the histone catalogue, the
source is the data.  Software tools to distribute new releases of
software, triggered by changes to the program can be configured to
trigger a build of the catalogue.  This would allow the biochemist
without bioinformatics aspirations to download the latest version.

\section{Data not in Flux}

By using data already made public by others, we side stepped the issue
of making our own data public.  This however, does not seem to be an
issue for many data types as there are dedicated repositories for
microarrays, sequencing, mass spectrometry, and flow cytometry.
Databases for microscopy data are much more limited and tailored to
specific groups \todo{check tools on 2016 Image Data Resource paper}.

For the microscopy data we acquired in \Cref{ch:kill-frap} we planned
to have a public OMERO server where we would host our data and we
could share with others.  With the infrastructure provided to us from
e-INIS, Irish National e-Infrastructure, a project to provide the
Irish research community with computational, network, and support
infrastructure,  we setup an instance of an OMERO server for use by the
microscope users at NUIG's CCB.  This was planned to provide the
centre with a centralised location for sharing the data between each
others, and possibly with the outside community.  However, few users
trialled the system and it did not achieve the uptake we would like.
In the end, the e-INIS project was closed and we
shutdown the server.

Even if e-INIS project had not terminated, maintenance of the service
for the rest of the group was not trivial.  There would have been need
for a specialised system administrator.  For sequence data,
centralised services exist.  NCBI and EMBL provide users with a
location to share their sequencing data, and even provide tools for
easy search and collaborative annotation.  But imaging data is another
beast, riddled with issues of proprietary formats, and of much larger
size and thus a much higher cost.

\todo[inline]{
There is zenodo now which seems promising.  They have a limit of 50GB
per dataset but we have 110GB of data which is probably not an issue
since it seems like we can just ask them for more.  We coudl also have
multiple datasets though.  It has the problem that it is just a dump
side for data, there is no added value, and no search for features
like there is for sequences.
I think zenodo didn't exist at the time and we didn't
investigate it either.}

\section{Reproducing Runtime Environment}

In computational research, one of the issues is the reproducibility of
runtime environment.  At a most simple level, it may seem like a
matter of installing the required software.  However, the
interdependency between software at all levels can have an impact on
the results.  Even hardware can be an issue although higher levels
languages such as Python, R, and Octave maintain an abstraction from
hardware that reduces such issues.

A proposal has been the distribution of Virtual Machines which
reproduces the environment.  However, this seems to be redundant to
the repositories of the GNU/Linux distributions they are based on.
There has been a recent project to make the build of all packages in
Debian reproducible and the details for that build are
\texttt{buildinfo} textfile with all the dependencies involved in the
build.  This could greatly simplify the management of such VM.  Only
a copy of the repositories, which would be shared between the VM would
have to be stored and the VM would be reduced to the buildinfo.
This would however require for all the used software to be packaged.

The creation of reproducible package builds is also not a trivial and
still a work in progress.

One other suggestion has been the creation of executable publications
such as Collage environment but this has seen little adoption.

\section{Version Control}

The goal of version control is to track changes to files which is
distinct from the goal of reproducing analyses.  As we used \LaTeX for
writing our manuscript, we were able to also use version control for
the writing of the manuscript.  One of the main reasons to use version
control is help with the merging of changes between multiple authors
editing the same document in parallel.  However, the tools for
handling the merging are specialised for source code and work on
lines.  This is very different from text where the difference should
happen in sentences.  The creation of merge tools for text could
simplify this greatly.  But it's not too bad, the needs for version
control in text are not the same for code, we don't go back looking
for who to blame for a specific line of code, or wondering why that
line is there and hope for a commit message more meaningful or the
context of the commit as one would do for analysis code.

Even if such tool was available, the concept of a manuscript as a text
file that is compiled into a document, the syntax of \LaTeX, and the
alternative formats are a lot of new concepts.  Interestingly, this is
exactly the only topic with two recommendations in the ``Good Enough
Practices in Scientific Computing'' to recognise the difficulty of
this tools, having been pushed by the reviewers.

The availability of the history of a project, made public in a such a
way could also be used for meta-research.  It would have data on the
method of writing scientific publications.

\section{Tools for Software Engineering}

We faced many issues with this project due to our lack of training in
the area.  Biologists do not typically receive the training for this
type of work during their studies.  Our feeling was that all the
problems had a solution that was a obvious to software engineers but
it was plain alien to a biology laboratory

This things are not new, a build system, version control, test units,
and reproducible builds is the field of software engineering.  But
biologists are still catching up.  This is why there's new
organisations like software carpentry and data carpentry, who are not
researching new methods to teach us, they are just teaching us the
methods that are already practised in software engineering.

How far can we push the use of tools for software engineering?  Can
bug trackers, tools to report issues with software and track the
development of their fixed, be used by other researches to report
reproducibility issues or feature requests to the original developers?
