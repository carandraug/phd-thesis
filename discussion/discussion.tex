%% \epigraph{tl;dr}{}
%% \epigraph{when the going gets tough, the tough get cardboard sleeves because the
%%           cups too hot.}{Doghouse diaries #5051}

Metaresearch, or Research on research,
is a very topical area and has become widely
discussed after a series of recent
studies showing low reproduciblity rates for
published research \citep{ioannidis2015meta}.  The
most extensive of these reproducibility studies have been in the field of
psychology \citep{open2015estimating},
computational science \citep{collberg2016repeatability},
and cancer biology \citep{prinz2011believe, ioannidis2009repeatability}.
They identified the lack of data availability
and details on methodology in analysis steps as
the main causes of difficulties in reproducibility.

In this thesis we attempted to follow the best practices for
reproducible computational analysis of biological data.
Central to our approach was the automation of the data analysis using
software build utilities.  These tools require explicitly declared
instructions on how to generate a certain file or reach a specific
goal such as performing analysis on a dataset to generate a
figure for a manuscript.  Having such a system in place ensures
reproducibility, makes the analysis transparent, and enables
other researchers to build upon the work more easily.

We used SCons, a software build system, to automate the analysis
steps of all our data.
Even so,
this still requires a researcher wishing to reproduce
our results to install all the required software and we recognise that
this is not a
trivial task.  To support this challenge, we included
configure scripts which are run at the start of the build system and
identify missing tools or features.
GNU/Linux distributions in turn ease
the task of installing the multiple tools by providing a
package manager, so we have also contributed to the packaging of
some of these required software components for the
widely used Debian distribution.

Finally, once the methods are available, access to the
data is required to reproduce the analysis.  In our case, we applied
our approach to both data from public sequence
databases \Crefp{ch:histone-catalogue} and to our own experimental
microscopy image data \Crefp{ch:kill-frap}.  For
the sequences in \Cref{ch:histone-catalogue} the usefulness is
enhanced because our analysis can be rerun after improvements in the
source data as well as for repeatability of results.
Nevertheless, we made available a
snapshot of the data used to build this thesis
for historical validations.
For \Cref{ch:kill-frap}, we made the
data publicly available in a research data repository.

This setup has enabled us to generate thesis and manuscript documents where each
figure and table can be traced directly back to the raw
data and the code used to generate them is freely
available.
In
\Cref{ch:histone-catalogue}, references to values are inserted in the
text as part of the analysis to avoid loss of validity as the data
changes and the analysis improves.  In \Cref{ch:kill-frap}, the
individual images, regions, and scaling options used to create the
inset figures are all explicit in code.  Even in the Introduction and
Discussion for both chapters, we
automated the generation of all figures.  For example,
\fref{fig:intro:histone-fold-domain} can be regenerated from the
crystal structure, as we include PyMOL
scripts for their creation, as is
the Ti\textit{k}Z source code
for \LaTeX{} to generate \fref{fig:intro:frap-curve-example} so that
others can improve it.

Only one exception to complete transparency
exists in this thesis.  Figures in
\Cref{ch:kill-frap} from the DeltaVision microscope were deconvolved
automatically after
acquisition by SoftWoRx which is non-free software.
As this was the very first step
after image acquisition, we trace all the steps from these
deconvolved images.

%% \todo{There's one other exception, the FACS figure
%%   in the methods chapter.  At the time I couldn't make the figure
%%   myself, but now I found an R package to read the facs file.  There
%%   is also a python package but it's abandoned.  Or we could just
%%   remove the figure, it's not that important anyway.}

\section{Data in Flux}

We found that our Histone Catalogue \Crefp{ch:histone-catalogue}
was particularly well suited
to this approach to transparent generation of a manuscript from
original data.  Not only are all the steps of a computational
nature without any wet-lab work, but the transitory nature of sequence data
makes the automated implementation of reproducible research useful
beyond the intellectual of achievement making it reproducible.

When we started the Histone Catalogue project, the last publication of
an equivalent catalogue
was from 2002 and was becoming outdated.  We initiated
this project with the aim of making a catalogue that would be
constantly up to date.  One year later in 2011, the Histone Database
\url{http://research.nhgri.nih.gov/histones/}
website with an up-to-date database specialised for histones
was released \citep{histonedb-2011}.
However, no update was made to that database until 2016 when a version 2.0
was released \citep{HistoneDB2016}.
We assume that further updates will
be infrequent and dependent on funding.
The Histone Database is a manually curated database of histone
genes of all species so it holds the same human sequences as we have
catalogued.
However, it does not have the extent of analysis of our catalogue.
While the Histone Database reflects a fixed point in time,
its manual curation could give a higher quality database.

Our automated
approach is inferior in principle because it is dependent on the
annotation of the existing genes in public repositories
while the Histone Database uses searches for
sequences with specific predicted structural motifs followed by manual
curation.  However, as long
as the logic behind the structural motif prediction can be encoded,
our Histone Catalogue could be adapted to perform the same search as
part of the build.
Alternatively, the Histone Catalogue could continue being built using
annotations as a representation of what is currently
recognised.  We created supplementary \tref{tab:curation-anomalies} to
list differences between the current model and annotation
with the purpose of identifying anomalies and
either improving the model or the
annotation.
Anomalies to the annotation can be fed back to RefSeq, as we have done
many times since we started our Histone Catalogue project.

The main point of our approach is that readers will have access to the
code.  They will not be limited to downloading the sequences we found
at a certain time, since
they can get the sequences at their point in time and can modify the
code to perform alternative analyses or use alternative data sources.
The design of our catalogue is
modular such that the choice of source data is just a block on
which the rest of the
analysis sits.  Similarly, all tables and figures are individual
blocks, so new tables and figures could be added to a new type of catalogue.
By making the whole code base public we are providing the full opportunity
to expand it.

The initial purpose of the Histone Catalogue
was to tabulate all human histone genes and the differences between
their coding sequences.  With time, we added other figures and became
curious about the unique regulatory elements of histones which
led to us adding an option to download downstream sequences in the
Bio-EUtilities code and then to the identification of multiple regulatory
elements that were not annotated in the RefSeq sequences.  We were
happy to find NCBI responsive to such corrections and they made
our submissions live for all RefSeq users.

We also tested the advantage of modifying the code ourselves for new
data sources.  The original catalogue was limited to human genes but it
failed when applied to the analysis of the mouse genome.  We changed
the software catalogue to make it organism independent and have
demonstrated its functionality by building a complete set of figures
for mouse canonical core histones
\Arefp{ch:mouse-catalogue}.  We envision that a similar catalogue
could be generated for other gene families by adapting the code base.

\section{Data not in Flux}

By using sequence data already available in public sources for the
Histone Catalogue, we avoided the issue
of making our own primary research results public.
Public repositories dedicated to scientific data
for multiple types of information have already been created such as GenBank for
nucleotide sequences, Protein Data Bank (PDB) for the structure of
molecules, and FlowRepository for flow cytometry.  However, databases
specialised for light microscopy are still not available
\citep{image-data-need-home}.

We faced this issue for the FRAP data
we acquired in \Cref{ch:kill-frap} and originally solved it by deploying
a public OMERO server, a system designed for storage and sharing of
biological data, specialised for microscopy \citep{omero}.
The infrastructure was provided by
e-INIS, the Irish National e-Infrastructure project to support the
Irish research community.
This was planned to provide our Centre for Chromosome Biology (CCB)
with a centralised location for sharing data between researchers
and possibly with the outside community.
However, few CCB users
trialled the system and it did not achieve the uptake we would like.
Ultimately, the e-INIS project was closed, and left without the
required infrastructure we
shut down the server.  This left us again without a platform for
sharing our microscopy data, and reflects an unexpected challenge for
the reproducible research approach.

While there is no repository for microscopy data, there are general
purpose unstructured repositories for research data.  BioStudies is a
database hosted by EMBL--EBI to collect data associated with
scientific publications \citep{mcentyre2015biostudies} while Zenodo
hosted at CERN provides a ``catch-all repository'' for research data.
In the end, we made all the data used to build this thesis
available for download on Zenodo and
DOI \texttt{10.5281/zenodo.377035}.
In addition to the microscopy data, and in the
interest of replicability, we also included the snapshot of histone
sequences used to build this thesis.

This experience also exposed the issue of long-term stability of
resources.  If the e-INIS project had terminated after completion of
the thesis, the dataset would no longer be available for future
readers.  Even if the e-INIS project had not come to an end, maintenance
of the service for the rest of the user group was not trivial so there
would have been the need for a specialised system administrator when the
project ended.  This means that preserving data for posterity may require
a party with large and long term resources such as NCBI, EMBL--EBI, or CERN.

\section{Runtime Environment}

In computational research, a major challenge to
reproducibility is
runtime environments.  At the most simple level, it may seem that
computation is a
matter of installing the required software but this step alone can
already be quite complicated for
scientific software.
However, the
interdependency between software at all levels can also have an impact on
the results.  Even hardware can be an issue, although higher levels
languages such as Python, R, and Octave provide an abstraction from
hardware that reduces this.

One proposal has been the distribution of Virtual Machines (VM) with
the entire stack of tools required to reproduce an
analysis \citep{hurley2015virtual, angiuoli2011clovr}.
However, this is effectively redundant with
the repositories of the GNU/Linux distributions they are based on, so VMs
are very wasteful in storage space.  It also misses an important point of
the reproducibility in transparency of how the system was built.

In free software distributions such as Debian, it is possible to
specify the build environment in a machine readable format that can be
reused later to reproduce the environment.  In the case of Debian,
the \texttt{.buildinfo} control file encodes all packages, with
version details, used to perform a build, as well as hardware
architecture, and a file checksum of the result for validation.  This
could greatly simplify the management of VMs, as only a single
copy of the distribution repositories would have to be stored.
However, this requires all software to be packaged.

We have packaged the dependencies for our Histone
Catalogue in Debian.  We have also added configure scripts which run
before the build and check for installed dependencies.  In addition,
we also created a test suite for our Histone Catalogue to confirm that
the environment works as expected while at the same time supporting our own
development process by preventing regressions.

\section{Continuous Delivery}

While researchers can, and we hope will, continue to develop and use our
Histone Catalogue approach, the level of skill required to reproduce it is not
common in the biochemistry and cell biology community.
Without a local bioinformatician, the
usefulfness of this power can be lost.  This means that training
opportunities and encouragement needs to be provided to increase the
relevant skills of typical molecular cell biologists if the vast
resources of data available are to be fully used.

For situations where data is in
flux as is the case of unversioned public databases such as RefSeq
used in our Histone Catalogue, the software
approach of continuous delivery could be applied to sequence data
analysis.  In this approach,
software is developed and is always in a state that can be
released.  Instantaneous releases are automated and anyone can download the
release with the latest change.  In the case of the Histone Catalogue,
changes to the data could be configured to
trigger an updated
build of the sequence catalogue.  This would allow a biochemist
without bioinformatics aspirations to download the latest analysis as
the manuscript PDF.

Several tools already exist to perform such automatic builds such as
Buildbot or Jenkins.  While our Histone Catalogue is not currently
triggered by data updates, distributing it via a public build server
that is triggered by a new RefSeq release, roughly every two months,
is an interesting future project that would make it more useful to the
wider research community.

The same approach and tools could be used for an ongoing project as
new data is being acquired.  In our project for
estimating the effect of point mutations on the dynamics of histone
proteins by FRAP, new images of FRAP experiments could trigger
their analysis to refine the
estimates obtained from the previous data.  Similarly, if we change
the FRAP model, for example, to account for an artefact
discovered as part of the research, this would trigger re-analysis of
all previous data.  Even automatic comparison between new mutations
could be automated in the same manner.
This approach can be useful as the project
advances and provide a live indication of progress as the results become more
accurate.

\section{Version Control}

The entire development of this thesis is available online as a git
repository at \url{https://github.com/carandraug/phd-thesis}.  The use
of a version control system provides access not only to the thesis at the
point of submission, but also to its entire history.
This is of less use
in a PhD thesis which was written only at the end of the
project.
However, both \Cref{ch:histone-catalogue} describing the Histone
Catalogue, and \Cref{ch:kill-frap} describing our approach to histone
dynamics by FRAP,
have been written as an individual
manuscript with the same ideals.

The goal of version control is to track changes to files, which is a
distinct task from the goal of reproducing the complete analyses.
However, version control provides extra information about a project to
anyone interested in continuing it by exposing the reasons for the
choice of methods implementation.  For example, the core
representation of an histone gene,
which is central to the Histone Catalogue project, has gone through
three major revisions and individual analysis scripts were constantly
modified.  A version control system associates each change with a
message so that each line of the analysis can be traced back in
history to an explanation about why it was done.  We used this
feature multiple times while developing the Histone Catalogue when a
piece of existing code raised questions about the analysis or interpretations.
This is quite common and version control systems will have a command
that annotates each line with the version that
introduced it.

As we used \LaTeX{} for
writing our chapters, we were also able to use version control for
the writing of the manuscripts.
Overall, this collaborative authoring process was very successful.
One of the main reasons for this
is the support version control
provides for merging of changes between multiple authors
editing the same document in parallel.  However, the tools for
handling the merging are specialised for source code and work on
lines, which is very different from text where differences
happen in sentences.  The creation of merge tools for text could
simplify this greatly by making changes easier to track and commit
messages more meaningful to the authoring process.

Even if such tools were available, the concept of a manuscript as a text
file that is compiled into a document, the syntax of \LaTeX{} or even
alternative markup formats are all alien concepts in biology.
In a recently submitted paper describing ``Good Enough
Practices in Scientific Computing'' \citep{good-enough-2016-v2}, the
authors also recognised this issue, and so provide an alternative
recommendation of using an online collaborative word processor.  While
this satisfies having a central master document accessible to all
authors, it would prevent the automatic insertion of up to date
figures and values as we do in our Histone Catalogue.

The availability of the history of a project through version control
could also be used for meta-research because it would provide data on the
method of writing scientific publications.

\section{Software Engineering and Biology}

We faced many issues with this project due to our lack of knowledge in
software engineering.  Biologists do not typically receive the training for this
type of work, although our observation is that almost all our
problems had solutions that were obvious to software engineers.
Concepts such as a build system, version control, test units,
and reproducible builds are not new in the field of software engineering but
biologists are still catching up.  This is why new
organisations like software
carpentry \url{https://software-carpentry.org/} and data
carpentry \url{http://www.datacarpentry.org/} have been established to
teach researchers
methods and perspectives that are already practised in software engineering.

\section{Conclusion}

While ultimately we concluded that FRAP
is unsuitable for our original research on histone contribution to
nucleosome stability,
we developed
and improved a multitude of tools for other researchers.  The Octave
FRAP package provides researchers with a flexible environment FRAP
analysis, and the work in Octave to support N dimensional images
provides support for the new imaging modalities.

In addition, we created a catalogue of all human canonical core
histones, and by handling the manuscript as a software project we hope
to have created the basis for future projects.  For example, our
Histone Catalogue could be ``forked'' for an alternative gene family.
Our lab had previously manually generated a catalogue of the Snf2
subfamilies \citep{andrew-snf2-catalogue} which could be improved in
the same manner.

Overall, a major finding is that it is possible to implement
manuscripts and entire thesis by following the reproducible research
approach for the computational and analysis phases of chromatin
research.  This openness has a number of important advantages for
reproducibility and progress in the quantitative molecular cell biology.
