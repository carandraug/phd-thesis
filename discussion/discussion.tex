%% \epigraph{tl;dr}{}
%% \epigraph{when the going gets tough, the tough get cardboard sleeves because the
%%           cups too hot.}{Doghouse diaries #5051}

Research on the actual reproducibility of the existing research is
rare and there's few publications.  This new field of meta research is
new but their initial publications are show that most researchers are
unable to reproduce results of others, and many are even unable to
reproduce their own research, and that the data or code used in
publications cannot be found.

There isn't much to go for, most of evidence is anecdotal but
reproducibility seems an issue.  Most researchers complain about it,
and journals are strating to act.

In this thesis, we tried to follow the best practices of this.  Every
single figure can be traced back to the very raw data and generated by
anyone else (with the exception of the facs plot -- how much free
software is there for facs?).  We provide all of the code required to
generate the data too, and more important, the code to run the
required code, the build system of this document.

\url{https://github.com/carandraug/phd-thesis} \todo{make a clone here}

In addition to the whole thesis, \Cref{ch:histone-catalogue} has been
written as an individual manuscript with the same ideals and is
available online at \url{https://github.com/af-lab/histone-catalogue}
\todo{make public}.

When we started the histone catalogue project, the last publication on
the subject was from 2002 and was becoming outdated.  We initialized
this project with the aim of making a catalogue that would not get out
of date ever.  One year later, the Histone database was published, a
website with an up to date database specialized for histones.
However, no update has been done until 2016 when the version 2.0 of
the database was released.  We can only guess that no update will be
done in the next few years, dependendent on them having receveied
funcind.

This is a manually curated database of the histones, at a specific
point in time.  However, in between releases, there is no other
access.  Our approach is also inferior.  We are dependent on the
annotation of the existing genes while their database looked for
sequences with specific predicted structural motifs.

The main point of our approach is that readers will have access to the
code.  They are not limited to download the sequences we found, they
ca get the sequences as they are now, and they can modify the code to
perform other things.  The build system generates botha manuscript and
a catalogue, and the catalogue can be easily added a new table and new
scripts can be written.  The idea can be adapted to other gene
families with arguibly more elegant aproaches on the search.

Our catalogue had served other purpose.  There is a set of rules of
what a gene is expected to have.  They have special downstream
regulatory elements, should not encode multiple products.  We could
compare this expectations with the real annotation and identify
anomalies, both in our assumptions and the annotation.  We were happy
to fin that the NCBI is quite responsive to corrections and have
accepted many changes we sent their way.

While researches can, and we hope they will, work on top of our
histone catalogue, the level of skill required to reproduce it is not
common on a cell biologist.  Without a local bioinformatician, the
usefulfness of this power is lost.  For situations where data is in
flux as is the case of unversioned public databases, the software
principle of continuous delivery could be applied.  In this concept,
software is being developed and is always in a state that can be
released.  The releases are automated and anyone can download the
release from the last change.  Compared to the histone catalogue, the
source is the data.  Software tools to distribute new releases of
software, triggered by changes to the program can be configured to
trigger a build of the catalogue.  This would allow the biochemist
without bioinformatics aspirations to download the latest version.

Our dependency on the annotation means we that the catalogue does not
work.  We have an option to use alternative genome assemblies. But it
still means that we can't work with genomes that are still a work in
progress.  An interesting work would be to replace the bottom of the
project with alternative source of genes, and see what happens.  Our
modular approach simply expects the data to be in a specfific
organization, three directories, one for genes, transcripts, and
proteins, and the files named after their gene unique ids or accession
numbers.

How far can we push the use of tools for software engineering?  Can
bug trackers, tools to report issues with software and track the
development of their fixed, be used by other researches to report
reproducibility issues or feature requests to the original developers?



\subsection{Version Control}
The code and text was under revision control, we can see how it was
written overtime.  We can see how we changed the figures

Not all was roses.  git is complicated.  We started with subversion
and we never had any conflict but that was because we didn't stress
test it.  We are sure that if it ever came to it, we would have just
the same merge problem.

\LaTeX may allow itself better than proprietary formats for version
control but it's still lousy for version control and diff tools which
work in lines. if a one word is changed on every line, it's difficult
to see what changed.  But it's not too bad, the needs for version
control in text are not the same for code, we don't go back looking
for who to blame for a specific line of code, or wondering why that
line is there and hope for a commit message more meaningful or the
context of the commit.

I wonder how much research could be done on the writing habit of this.
Correlation of how much writing is done with the near of deadlines for
grants or thesis.  Do different authors spend different times in
different parts of a manuscript?

\todo[inline]{histones website 1.0 was out of date until 2.0 came out,
  5 years later.  In the mean time it was out of date.  Power to the
  users.  This is aporblem on the standrad style of databses, the
  people need the code.  We can't be dependent on other shwo have
  better things to do.}

\todo[inline]{this whole thesis is reproducible, all figures.  Even
  the introduction}
\todo[inline]{problems of dependencies. We used a configure script}

\todo[inline]{sequences in Gene RefSeq, no id for annotation changes.
  Only possible to revert to specific releases.}

\todo[inline]{for images, there's IDR.  Can future be federated? We
  had omero but then we ran out of sysadmin.  A facility needs to
  provide more.  A university needs to provde more than just a dump of
  files}

\subsection{The required skillset}

We faced many issues with this project due to our lack of training in
the area.  Biologists do not typically receive the training for this
type of work during their studies. 



\subsection{Data}

Our approach is not perfect.  We do not address the issue of sharing
the data.  In the case of our catalogue of core histones, the data is
a state of flux and available in public databases.  We have no control
over it and that's the power of it.

However, for our studies in microscopy the data is ours.  We planned
to have a public OMERO server where we would host our data and we
could share with others.  With the infrastructure provided to us from
e-INIS, Irish National e-Infrastructure, a project to provide the
Irish research community with computational, network, and support
infrastructure,  we setup an instance of an OMERO server for use by the
microscope users at NUIG's CCB.  This was planned to provide the
centre with a centralised location for sharing the data between each
others, and possibly with the outside community.  However, we were the
only users of the system, no one else on the site used it for more
than a testing.  In the end, the e-INIS project was closed and we
shutdown the server.

Even if e-INIS project had not terminated, maintenance of the service
for the rest of the group was not trivial.  There would have been need
for a specialised system administrator.  For sequence data,
centralised services exist.  NCBI and EMBL provide users with a
location to share their sequencing data, and even provide tools for
easy search and collaborative annotation.  But imaging data is another
beast, riddled with issues of proprietary formats, and of much larger
size and thus a much higher cost.

There is zenodo now which seems promising.  They have a limit of 50GB
per dataset but we have 110GB of data.  We can have multiple datasets
though.  The truth is that didn't exist at the time and we didn't
investigate it.  This seems to be of the most use when we actually
publish it.

\todo[inline]{check IDR paper.  They mention a few resources but they
  sound for a very selected use case.}


we have had a
few users trialling the system. However, it has not achieved the
uptake or impacts we would have liked yet




give imagej as example of an interactive application, not designed to
be run alone. Macros make it better.


  but this has several issues, mainly the difficulty of programming
  for scientists which have not been trained for such tasks. Even if
  the source code is made available, there is no knowledge to modify it.


computers -> more data -> harder to analyse



reproduciblity has two reasons, so that others can reproduce.  On a
real reproducible enviornmenr, this owuld not be an issue, manuscripts
would be rebuilt automatically, and the tools are to prevent issues
about special hardwarre requirements or local configurations.  The
prevention of fraud would not even be regisered.

In software, reproducibility and build system are also used by the
same person to test in other systems.  As a comparison, we also tested
our catalogue in mice and found a series of bugs, needed to make it
organism independent.  Wrong asumptions on human, such as the
nomenclature for genome or that gene symbols were all upper case.

\todo[inline]{last paragraph in the style of a conclusion}
