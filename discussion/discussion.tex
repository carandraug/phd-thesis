%% \epigraph{tl;dr}{}
%% \epigraph{when the going gets tough, the tough get cardboard sleeves because the
%%           cups too hot.}{Doghouse diaries #5051}

Research on research is a very topical area, after a series of recent
studies showing low reproduciblity rates for published research\addref{}.  The
biggest studies have been on the field of psychology and
computational science, but the same trend appears to be in the biomedical
sciences\addref{}.  The main cause for difficulties with
reproducibility has been the lack of availability of
data and lack of details on methodology in analysis steps.
In the field of computational science, the main issue has been the absence of
availability
of the code developed \addref{}.

In this thesis, we attempt to follow the best practices for
reproducibility.  Figures and tables can be traced back to the raw
data and the code used to generate them has been made freely available.  In
\Cref{ch:histone-catalogue}, references to values are inserted in the
text as part of the analysis to avoid loss of validity as the data
changes and our analysis improves.  In \Cref{ch:kill-frap}, the
individual images, regions, and scaling options used to create the
inset figures are all made public.  Even in the introduction and
explanatory sections, we
automated the generation of all figures.  For example,
\fref{fig:intro:histone-fold-domain} can be regenerated from the
crystal structure, as we include PyMOL \addref{} scripts for their creation, and
\fref{fig:intro:frap-curve-example} has the Ti\textit{k}Z source code so that
others can improve it.  Only one exception exists in this thesis.  Figures in
\Cref{ch:kill-frap} from the DeltaVision microscope were deconvolved after
acquisition by SoftWoRx which is non-free software.  As this was the
very first step after image acquisition, we trace all the steps to the
deconvolved images.

%% \todo{There's one other exception, the FACS figure
%%   in the methods chapter.  At the time I couldn't make the figure
%%   myself, but now I found an R package to read the facs file.  There
%%   is also a python package but it's abandoned.  Or we could just
%%   remove the figure, it's not that important anyway.}

The entire development of this thesis is available online as a git
repository at \url{https://github.com/carandraug/phd-thesis}.  The use
of a version control system provides access not only the thesis at the
point of submission, but also to its entire history.  This is of less use
in a PhD thesis which is started and written only at the end of the
project.

However,
\Cref{ch:histone-catalogue} has been written as an individual
manuscript with the same ideals and is online at
\url{https://github.com/af-lab/histone-catalogue}.  The core
representation of an histone gene in that project has gone through
three major revisions and individual analysis scripts were constantly
modified.  A version control system associates each change with a
message so that each line of the analysis can be traced back in
history to an explanation on why it was done that way.

In addition, we used a software build system (SCons) to automate the analysis
steps with all options specified.  This still requires a research
wishing to reproduce
our results to install all the required software which is not a
trivial task.  To identity this issue as early as possible, we include
configure scripts which are run at the start of the build system and
identify any missing tool or feature.  GNU/Linux distributions ease
the task of installing the multiple tools required by providing a
package manager so we have contributed to the packaging of
some of these requirements for the Histone Catalogue case based on the
widely used Debian distribution.

\todo[inline]{Could you say some about \Cref{ch:kill-frap}? This is
  built in such a way that you or another PhD student could hopefully
  add to it for a formal publication}

\section{Data in Flux}

We found that our Histone Catalogue project was particularly well suited
to this approach.  Not only are all the steps of a computational
nature without any wet-lab work, but the transitory nature of sequence data
makes the automated nature of the reproducible research approach interesting
beyond the intellectual of just making it reproducible.

When we started the Histone Catalogue project, the last publication on
the subject was from 2002 and was becoming outdated.  We initiated
this project with the aim of making a catalogue that would be
constantly up to date.  One year later in 2011, the Histone database
\todo{wwww....}
website with an up-to-date database specialised for histones
was released \addref{}.
However, no update was made until 2016 with the version 2.0 of
the database\addref{}.  We can only assume that update will typically
be infrequent and dependent on funding.
The Histone Database is a manually curated database of histone
genes for all species.  While this means it reflects a fixed point in time,
the manual curation potentially enables an higher quality database.
Our automated
approach is inferior in principle because it is dependent on the
annotation of the existing genes in public repositories
while the Histone Database is based on searching for
sequences with specific predicted structural motifs.  However, our
Histone Catalogue could be adapted to a search by
encoding the search logic as part of our build system.

The main point of our approach is that readers will have access to the
code.  They will not be limited to downloading the sequences we found
at a certain time since
they can get the sequences at their time point and can modify the
code to perform alternative analysis.
The design of our catalogue was
modular such that the source of data is just a block where the rest of the
analysis sits.  Similarly, all tables and figures are individual
blocks, so new tables and figures could be added to a new catalogue.
By making the whole code public we are providing the full opportunity
to expand it.

The initial purpose of the Histone Catalogue
was to tabulate all human histone genes and the differences between
their coding sequences.  With time, we kept adding other figures and became
curious about the histone unique regulatory elements which
led to the addition of an option to download downstream sequences in
Bio-EUtilities and then to the identification of multiple regulatory
elements that were not annotated in the reference sequences.  We were
happy to find NCBI responsive to such corrections and they made
our submissions live for all RefSeq users.
We then created a new table that compares
the current annotations with the existing assumptions of histone genes
structures and generates a list of current anomalies.

We also tested the advantage of modifying the code ourselves for new
purposes.  The original catalogue was limited to human genes but it
failed when attempted to analyse the mouse genome.  Therefore, we changed
the catalogue to make it organism independent (see
\Aref{ch:mouse-catalogue}).  We envision that a similar catalogue
could be generated by adapting the code base to other gene families.

While researches can, and we hope they will, work on top of our
Histone Catalogue, the level of skill required to reproduce it is not
common in the cell biology community.  Without a local bioinformatician, the
usefulfness of this power is lost.  This means that training
opportunities and encouragement needs to be provided to increase the
relevant skills of typical molecular cell biologists if the vast
resources of data available are to be fully used.
For situations where data is in
flux as is the case of unversioned public databases such as RefSeq
used in our project, the software
principle of continuous delivery could be applied to sequence data
analysis.  In this concept,
software is developed and is always in a state that can be
released.  Instantaneous releases are automated and anyone can download the
release with the latest change.  Compared to the Histone Catalogue, the
source is the data so software tools to distribute new releases
triggered by changes to the data can be configured to
trigger a build of the sequence catalogue.  This would allow the biochemist
without bioinformatics aspirations to download the latest analysis.
Our Histone Catalogue is not yet triggered by data updates but can be
scripted to run on defined cycles, or on demand.

\section{Data not in Flux}

By using sequence data already available in public by others,
we avoided the issue
of making our own ongoing research public.  This could also apply
issue for many data types as there are dedicated repositories for
microarrays, sequencing, mass spectrometry, and flow cytometry.
Databases for microscopy data are much more limited and tailored to
specific groups \todo{check tools on 2016 Image Data Resource paper}.

For the microscopy data we acquired in \Cref{ch:kill-frap} we planned
to have a public OMERO server where we would host our data and we
could share with others.  With the infrastructure provided to us from
e-INIS, the Irish National e-Infrastructure project to provide the
Irish research community with computational support
infrastructure,  we setup an instance of an OMERO server for use by
microscope users in our Centre for Chromosome Biology.
This was planned to provide the
centre with a centralised location for sharing the data between each
others, and possibly with the outside community.  However, few users
trialled the system and it did not achieve the uptake we would like.
Ultimately, the e-INIS project was closed and we
shut down the server.

Even if e-INIS project had not terminated, maintenance of the service
for the rest of the user group was not trivial so there would have been need
for a specialised system administrator.  For sequence data,
centralised services already exist.
For example, NCBI and EMBL provide users with a
location to share their sequencing data, and even provide tools for
easy search and collaborative annotation.  In contrast, imaging data
has challenges with proprietary formats and much larger dataset sizes
so presents a much higher cost.

\todo[inline]{Need 2 sentences explaining how this is relevant? I
  think you are saying that publishing chapter 4 has different
  challenges?}
\todo[inline]{I would like you to include a paragraph discussing how
  reproducible and updatable research could improve the FRAP project.
  For example by showing how more data or new models could converge on
  an accurate estimate for histone kinetics.}

\todo[inline]{
There is zenodo now which seems promising.  They have a limit of 50GB
per dataset but we have 110GB of data which is probably not an issue
since it seems like we can just ask them for more.  We coudl also have
multiple datasets though.  It has the problem that it is just a dump
side for data, there is no added value, and no search for features
like there is for sequences.
I think zenodo didn't exist at the time and we didn't
investigate it either.}

\section{Reproducing Runtime Environment}

In computational research, one of the largest challenges
is with the reproducibility of
runtime environments.  At a most simple level, it may seem that
computation is a
matter of installing the required software.  However, the
interdependency between software at all levels can have an impact on
the results.  Even hardware can be an issue although higher levels
languages such as Python, R, and Octave maintain an abstraction from
hardware that reduces this.

One proposal is the distribution of Virtual Machines (VM) which
reproduce the exact environment.  However, this seems to be redundant with
the repositories of the GNU/Linux distributions they are based on.
There has been a recent project to make the build of all packages in
Debian reproducible with the details for that build in a
\texttt{buildinfo} textfile with all the dependencies listed.
This could greatly simplify the management of such VM.  Only
a copy of the repositories, which would be shared between the VM, would
have to be stored and the VM would be reduced to the buildinfo.
This would require for all software to be packaged.
However, the creation of reproducible package builds is also not a trivial and
still a work in progress.

\section{Version Control}

The goal of version control is to track changes to files which is
distinct from the goal of reproducing analyses.  As we used \LaTeX{} for
writing our manuscript, we were also able to also use version control for
the writing of the manuscript.  One of the main reasons for this
is help with the merging of changes between multiple authors
editing the same document in parallel.  However, the tools for
handling the merging are specialised for source code and work on
lines which is very different from text where differences
happen in sentences.  The creation of merge tools for text could
simplify this greatly by making changes easier to track and commit
messages more meaningful to the authoring process.

Even if such tool was available, the concept of a manuscript as a text
file that is compiled into a document, the syntax of \LaTeX, and the
alternative formats are very new concepts in academia.  Interestingly, this is
exactly the only topic with two recommendations in the ``Good Enough
Practices in Scientific Computing'' to recognise the difficulty of
this tools, having been pushed by the reviewers \todo{?}.

The availability of the history of a project, made public in a such a
way could also be used for meta-research because it would have data on the
method of writing scientific publications.

\section{Tools for Software Engineering}

We faced many issues with this project due to our lack of training in
software engineering.  Biologists do not typically receive the training for this
type of work during their studies.  Our observation is that all the
problems had a solution that was a obvious to software engineers but
alien in a biology laboratory

Concepts such as a build system, version control, test units,
and reproducible builds are not new in field of software engineering but
biologists are still catching up.  This is why new
organisations like software carpentry and data carpentry have been
established to teach biologists the
methods that are already practised in software engineering.

This also opens the question of how much biologists implement
tools for software engineering.  For example, can
bug trackers, tools to report issues with software and track the
development of their fixed, could be used by other researchers to report
reproducibility issues or feature requests to the original experimenters.

\todo[inline]{Need another paragraph or two summing up the thesis?}
